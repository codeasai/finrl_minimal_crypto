{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Agent (Agent Creation)\n",
    "## ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á RL Agent ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Crypto Trading\n",
    "\n",
    "### ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢:\n",
    "- ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß\n",
    "- ‡∏™‡∏£‡πâ‡∏≤‡∏á Trading Environment\n",
    "- ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Agent Architecture\n",
    "- ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Hyperparameters\n",
    "- ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Agent ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Import Libraries ‡πÅ‡∏•‡∏∞ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Setup directories completed\n",
      "üìä Starting Data Preprocessing Process\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import ta  # Technical Analysis library\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Import config\n",
    "from config import *\n",
    "\n",
    "# Setup directories\n",
    "RAW_DIR = \"raw_data\"\n",
    "PROCESSED_DIR = \"processed_data\"\n",
    "\n",
    "if not os.path.exists(PROCESSED_DIR):\n",
    "    os.makedirs(PROCESSED_DIR)\n",
    "\n",
    "print(\"üìÅ Setup directories completed\")\n",
    "print(f\"üìä Starting Data Preprocessing Process\")\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading raw data...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'raw_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 76\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mload_raw_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m df \u001b[38;5;241m=\u001b[39m clean_data(df)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müìä Cleaned data summary:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m, in \u001b[0;36mload_raw_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müìÇ Loading raw data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# ‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡πÉ‡∏ô raw_data\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m csv_files \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRAW_DIR\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m csv_files:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo CSV files found in raw_data directory\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'raw_data'"
     ]
    }
   ],
   "source": [
    "def load_raw_data():\n",
    "    \"\"\"\n",
    "    ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV\n",
    "    \"\"\"\n",
    "    print(\"üìÇ Loading raw data...\")\n",
    "    \n",
    "    # ‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡πÉ‡∏ô raw_data\n",
    "    csv_files = [f for f in os.listdir(RAW_DIR) if f.endswith('.csv')]\n",
    "    if not csv_files:\n",
    "        raise FileNotFoundError(\"No CSV files found in raw_data directory\")\n",
    "    \n",
    "    latest_file = max(csv_files, key=lambda x: os.path.getctime(os.path.join(RAW_DIR, x)))\n",
    "    file_path = os.path.join(RAW_DIR, latest_file)\n",
    "    \n",
    "    # ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(df)} rows from {latest_file}\")\n",
    "    print(f\"üìÖ Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "    print(f\"üìä Symbols: {df['tic'].unique()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "    \"\"\"\n",
    "    print(\"üßπ Cleaning data...\")\n",
    "    \n",
    "    # ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡πâ‡∏≥\n",
    "    initial_len = len(df)\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"  Removed {initial_len - len(df)} duplicate rows\")\n",
    "    \n",
    "    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤\n",
    "    df = df.sort_values(['timestamp', 'tic'])\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    if missing_values.any():\n",
    "        print(\"\\n‚ö†Ô∏è Found missing values:\")\n",
    "        print(missing_values[missing_values > 0])\n",
    "        \n",
    "        # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç missing values\n",
    "        for col in df.columns:\n",
    "            if df[col].isnull().any():\n",
    "                if col in ['open', 'high', 'low', 'close']:\n",
    "                    # Forward fill ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏≤‡∏Ñ‡∏≤\n",
    "                    df[col] = df.groupby('tic')[col].fillna(method='ffill')\n",
    "                elif col == 'volume':\n",
    "                    # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏î‡πâ‡∏ß‡∏¢ 0 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö volume\n",
    "                    df[col] = df[col].fillna(0)\n",
    "    \n",
    "    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡πà‡∏≤‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥\n",
    "    numeric_columns = ['open', 'high', 'low', 'close', 'volume']\n",
    "    for col in numeric_columns:\n",
    "        q1 = df[col].quantile(0.25)\n",
    "        q3 = df[col].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "        \n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "        if len(outliers) > 0:\n",
    "            print(f\"\\n‚ö†Ô∏è Found {len(outliers)} outliers in {col}\")\n",
    "            # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡πà‡∏≤‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á\n",
    "            df.loc[df[col] < lower_bound, col] = lower_bound\n",
    "            df.loc[df[col] > upper_bound, col] = upper_bound\n",
    "    \n",
    "    print(\"‚úÖ Data cleaning completed\")\n",
    "    return df\n",
    "\n",
    "# ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "df = load_raw_data()\n",
    "df = clean_data(df)\n",
    "\n",
    "print(f\"\\nüìä Cleaned data summary:\")\n",
    "print(f\"  Total rows: {len(df)}\")\n",
    "print(f\"  Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "print(f\"  Symbols: {df['tic'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Technical Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_technical_indicators(df):\n",
    "    \"\"\"\n",
    "    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Technical Indicators ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ symbol\n",
    "    \"\"\"\n",
    "    print(\"üìà Calculating technical indicators...\")\n",
    "    \n",
    "    # ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡πÉ‡∏´‡∏°‡πà\n",
    "    processed_df = pd.DataFrame()\n",
    "    \n",
    "    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì indicators ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ symbol\n",
    "    for symbol in df['tic'].unique():\n",
    "        print(f\"\\nProcessing {symbol}...\")\n",
    "        symbol_data = df[df['tic'] == symbol].copy()\n",
    "        \n",
    "        # Trend Indicators\n",
    "        symbol_data['sma_20'] = ta.trend.sma_indicator(symbol_data['close'], window=20)\n",
    "        symbol_data['sma_50'] = ta.trend.sma_indicator(symbol_data['close'], window=50)\n",
    "        symbol_data['ema_20'] = ta.trend.ema_indicator(symbol_data['close'], window=20)\n",
    "        symbol_data['macd'] = ta.trend.macd_diff(symbol_data['close'])\n",
    "        \n",
    "        # Momentum Indicators\n",
    "        symbol_data['rsi'] = ta.momentum.rsi(symbol_data['close'], window=14)\n",
    "        symbol_data['stoch'] = ta.momentum.stoch(symbol_data['high'], symbol_data['low'], symbol_data['close'])\n",
    "        symbol_data['williams_r'] = ta.momentum.williams_r(symbol_data['high'], symbol_data['low'], symbol_data['close'])\n",
    "        \n",
    "        # Volatility Indicators\n",
    "        symbol_data['bb_high'] = ta.volatility.bollinger_hband(symbol_data['close'])\n",
    "        symbol_data['bb_low'] = ta.volatility.bollinger_lband(symbol_data['close'])\n",
    "        symbol_data['atr'] = ta.volatility.average_true_range(symbol_data['high'], symbol_data['low'], symbol_data['close'])\n",
    "        \n",
    "        # Volume Indicators\n",
    "        symbol_data['obv'] = ta.volume.on_balance_volume(symbol_data['close'], symbol_data['volume'])\n",
    "        symbol_data['vwap'] = ta.volume.volume_weighted_average_price(symbol_data['high'], symbol_data['low'], symbol_data['close'], symbol_data['volume'])\n",
    "        \n",
    "        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÅ‡∏•‡πâ‡∏ß\n",
    "        processed_df = pd.concat([processed_df, symbol_data])\n",
    "        \n",
    "        print(f\"‚úÖ Added {len(symbol_data)} rows with indicators\")\n",
    "    \n",
    "    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "    processed_df = processed_df.sort_values(['timestamp', 'tic']).reset_index(drop=True)\n",
    "    \n",
    "    print(\"\\n‚úÖ Technical indicators calculation completed\")\n",
    "    print(f\"üìä Total indicators: {len(processed_df.columns) - 6}\")  # ‡∏•‡∏ö 6 ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô\n",
    "    \n",
    "    return processed_df\n",
    "\n",
    "# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì technical indicators\n",
    "df = calculate_technical_indicators(df)\n",
    "\n",
    "print(f\"\\nüìä Indicators summary:\")\n",
    "print(f\"  Total columns: {len(df.columns)}\")\n",
    "print(f\"  Indicators: {[col for col in df.columns if col not in ['timestamp', 'tic', 'open', 'high', 'low', 'close', 'volume']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Normalize ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(df):\n",
    "    \"\"\"\n",
    "    Normalize ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏≤‡∏Ñ‡∏≤‡πÅ‡∏•‡∏∞ indicators\n",
    "    \"\"\"\n",
    "    print(\"üìä Normalizing data...\")\n",
    "    \n",
    "    # ‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á normalize\n",
    "    non_numeric_cols = ['timestamp', 'tic']\n",
    "    numeric_cols = [col for col in df.columns if col not in non_numeric_cols]\n",
    "    \n",
    "    # ‡∏™‡∏£‡πâ‡∏≤‡∏á scaler\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    # Normalize ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ symbol\n",
    "    normalized_data = pd.DataFrame()\n",
    "    \n",
    "    for symbol in df['tic'].unique():\n",
    "        print(f\"\\nNormalizing {symbol}...\")\n",
    "        symbol_data = df[df['tic'] == symbol].copy()\n",
    "        \n",
    "        # Normalize ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç\n",
    "        numeric_data = symbol_data[numeric_cols]\n",
    "        normalized_numeric = scaler.fit_transform(numeric_data)\n",
    "        \n",
    "        # ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡πÉ‡∏´‡∏°‡πà\n",
    "        normalized_symbol_data = pd.DataFrame(normalized_numeric, columns=numeric_cols)\n",
    "        normalized_symbol_data[non_numeric_cols] = symbol_data[non_numeric_cols]\n",
    "        \n",
    "        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà normalize ‡πÅ‡∏•‡πâ‡∏ß\n",
    "        normalized_data = pd.concat([normalized_data, normalized_symbol_data])\n",
    "        \n",
    "        print(f\"‚úÖ Normalized {len(symbol_data)} rows\")\n",
    "    \n",
    "    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "    normalized_data = normalized_data.sort_values(['timestamp', 'tic']).reset_index(drop=True)\n",
    "    \n",
    "    print(\"\\n‚úÖ Data normalization completed\")\n",
    "    return normalized_data\n",
    "\n",
    "# Normalize ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "df = normalize_data(df)\n",
    "\n",
    "print(f\"\\nüìä Normalized data summary:\")\n",
    "print(f\"  Total rows: {len(df)}\")\n",
    "print(f\"  Total columns: {len(df.columns)}\")\n",
    "print(f\"  Numeric columns: {[col for col in df.columns if col not in ['timestamp', 'tic']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df):\n",
    "    \"\"\"\n",
    "    ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô Training/Validation/Testing\n",
    "    \"\"\"\n",
    "    print(\"üìä Splitting data...\")\n",
    "    \n",
    "    # ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏°‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô 70:15:15\n",
    "    total_len = len(df)\n",
    "    train_size = int(total_len * 0.7)\n",
    "    val_size = int(total_len * 0.15)\n",
    "    \n",
    "    train_df = df.iloc[:train_size].reset_index(drop=True)\n",
    "    val_df = df.iloc[train_size:train_size + val_size].reset_index(drop=True)\n",
    "    test_df = df.iloc[train_size + val_size:].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"‚úÖ Data split completed:\")\n",
    "    print(f\"  Training set: {len(train_df)} rows ({len(train_df)/total_len*100:.1f}%)\")\n",
    "    print(f\"  Validation set: {len(val_df)} rows ({len(val_df)/total_len*100:.1f}%)\")\n",
    "    print(f\"  Testing set: {len(test_df)} rows ({len(test_df)/total_len*100:.1f}%)\")\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "def save_processed_data(df, train_df, val_df, test_df):\n",
    "    \"\"\"\n",
    "    ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß\n",
    "    \"\"\"\n",
    "    print(\"üíæ Saving processed data...\")\n",
    "    \n",
    "    # ‡∏™‡∏£‡πâ‡∏≤‡∏á timestamp\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
    "    pickle_file = os.path.join(PROCESSED_DIR, f\"processed_crypto_data_{timestamp}.pkl\")\n",
    "    with open(pickle_file, 'wb') as f:\n",
    "        pickle.dump(df, f)\n",
    "    print(f\"‚úÖ Saved full data to {pickle_file}\")\n",
    "    \n",
    "    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏¢‡∏Å‡∏™‡πà‡∏ß‡∏ô\n",
    "    train_file = os.path.join(PROCESSED_DIR, f\"train_data_{timestamp}.pkl\")\n",
    "    val_file = os.path.join(PROCESSED_DIR, f\"val_data_{timestamp}.pkl\")\n",
    "    test_file = os.path.join(PROCESSED_DIR, f\"test_data_{timestamp}.pkl\")\n",
    "    \n",
    "    with open(train_file, 'wb') as f:\n",
    "        pickle.dump(train_df, f)\n",
    "    with open(val_file, 'wb') as f:\n",
    "        pickle.dump(val_df, f)\n",
    "    with open(test_file, 'wb') as f:\n",
    "        pickle.dump(test_df, f)\n",
    "    \n",
    "    print(f\"‚úÖ Saved split data to:\")\n",
    "    print(f\"  - {train_file}\")\n",
    "    print(f\"  - {val_file}\")\n",
    "    print(f\"  - {test_file}\")\n",
    "    \n",
    "    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î\n",
    "    latest_files = {\n",
    "        'full': os.path.join(PROCESSED_DIR, \"processed_crypto_data.pkl\"),\n",
    "        'train': os.path.join(PROCESSED_DIR, \"train_data.pkl\"),\n",
    "        'val': os.path.join(PROCESSED_DIR, \"val_data.pkl\"),\n",
    "        'test': os.path.join(PROCESSED_DIR, \"test_data.pkl\")\n",
    "    }\n",
    "    \n",
    "    for key, file_path in latest_files.items():\n",
    "        if os.path.exists(file_path):\n",
    "            os.remove(file_path)\n",
    "        \n",
    "        if key == 'full':\n",
    "            df.to_csv(file_path.replace('.pkl', '.csv'), index=False)\n",
    "        else:\n",
    "            eval(f\"{key}_df\").to_csv(file_path.replace('.pkl', '.csv'), index=False)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Saved latest files to:\")\n",
    "    for key, file_path in latest_files.items():\n",
    "        print(f\"  - {file_path.replace('.pkl', '.csv')}\")\n",
    "\n",
    "# ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "train_df, val_df, test_df = split_data(df)\n",
    "\n",
    "# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "save_processed_data(df, train_df, val_df, test_df)\n",
    "\n",
    "print(f\"\\n   Data preprocessing completed successfully!\")\n",
    "print(f\"üìä Final data summary:\")\n",
    "print(f\"  Total rows: {len(df)}\")\n",
    "print(f\"  Total columns: {len(df.columns)}\")\n",
    "print(f\"  Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "print(f\"  Symbols: {df['tic'].unique()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
