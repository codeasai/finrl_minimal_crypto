{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô Agent (Agent Training)\n",
    "## ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô RL Agent ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Crypto Trading\n",
    "\n",
    "### ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢:\n",
    "- ‡πÇ‡∏´‡∏•‡∏î Agent ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ß‡πâ\n",
    "- ‡πÄ‡∏ó‡∏£‡∏ô Agent ‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Training\n",
    "- Validate ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Validation\n",
    "- ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Model ‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏•‡πâ‡∏ß\n",
    "- ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Learning Progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Import Libraries ‡πÅ‡∏•‡∏∞‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# FinRL imports\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# Import config\n",
    "from config import *\n",
    "\n",
    "# Setup directories\n",
    "PROCESSED_DIR = \"processed_data\"\n",
    "MODEL_DIR = \"models\"\n",
    "AGENT_DIR = \"agents\"\n",
    "LOGS_DIR = \"logs\"\n",
    "TENSORBOARD_DIR = \"tensorboard_logs\"\n",
    "\n",
    "for dir_name in [MODEL_DIR, LOGS_DIR, TENSORBOARD_DIR]:\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "\n",
    "print(\"üìÅ Setup directories completed\")\n",
    "print(f\"üöÄ Starting Agent Training Process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: ‡πÇ‡∏´‡∏•‡∏î Environment ‡πÅ‡∏•‡∏∞ Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞ environment\n",
    "def load_training_setup():\n",
    "    print(\"üìÇ Loading training setup...\")\n",
    "    try:\n",
    "        pickle_file = os.path.join(PROCESSED_DIR, \"processed_crypto_data.pkl\")\n",
    "        with open(pickle_file, 'rb') as f:\n",
    "            df = pickle.load(f)\n",
    "        print(f\"‚úÖ Loaded processed data from {pickle_file}\")\n",
    "    except:\n",
    "        csv_file = os.path.join(PROCESSED_DIR, \"processed_crypto_data.csv\")\n",
    "        df = pd.read_csv(csv_file)\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        print(f\"‚úÖ Loaded processed data from {csv_file}\")\n",
    "    env_config_file = os.path.join(AGENT_DIR, \"environment_config.pkl\")\n",
    "    with open(env_config_file, 'rb') as f:\n",
    "        env_config = pickle.load(f)\n",
    "    print(f\"‚úÖ Loaded environment config\")\n",
    "    agent_config_file = os.path.join(AGENT_DIR, \"agent_configs.pkl\")\n",
    "    with open(agent_config_file, 'rb') as f:\n",
    "        agent_configs = pickle.load(f)\n",
    "    print(f\"‚úÖ Loaded agent configs\")\n",
    "    agent_info_file = os.path.join(AGENT_DIR, \"agent_info.pkl\")\n",
    "    with open(agent_info_file, 'rb') as f:\n",
    "        agent_info = pickle.load(f)\n",
    "    print(f\"‚úÖ Loaded agent info\")\n",
    "    return df, env_config, agent_configs, agent_info\n",
    "\n",
    "def recreate_environments(df, env_config):\n",
    "    print(\"üèõÔ∏è Recreating environments...\")\n",
    "    total_len = len(df)\n",
    "    train_size = int(total_len * 0.7)\n",
    "    val_size = int(total_len * 0.15)\n",
    "    train_df = df.iloc[:train_size].reset_index(drop=True)\n",
    "    val_df = df.iloc[train_size:train_size + val_size].reset_index(drop=True)\n",
    "    test_df = df.iloc[train_size + val_size:].reset_index(drop=True)\n",
    "    for data in [train_df, val_df, test_df]:\n",
    "        data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "        data['date'] = data['timestamp'].dt.date\n",
    "        data.sort_values(['date', 'tic'], inplace=True)\n",
    "        data.reset_index(drop=True, inplace=True)\n",
    "    env_kwargs = env_config['env_kwargs']\n",
    "    train_env = StockTradingEnv(df=train_df, **env_kwargs)\n",
    "    val_env = StockTradingEnv(df=val_df, **env_kwargs)\n",
    "    test_env = StockTradingEnv(df=test_df, **env_kwargs)\n",
    "    train_env = Monitor(train_env, os.path.join(LOGS_DIR, \"train_monitor\"))\n",
    "    val_env = Monitor(val_env, os.path.join(LOGS_DIR, \"val_monitor\"))\n",
    "    print(\"‚úÖ Environments recreated and wrapped with Monitor\")\n",
    "    return train_env, val_env, test_env, train_df, val_df, test_df\n",
    "\n",
    "# ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á environments\n",
    "df, env_config, agent_configs, agent_info = load_training_setup()\n",
    "train_env, val_env, test_env, train_df, val_df, test_df = recreate_environments(df, env_config)\n",
    "print(f\"\\nüìä Training setup completed:\")\n",
    "print(f\"  Train data: {len(train_df)} rows\")\n",
    "print(f\"  Val data: {len(val_df)} rows\")\n",
    "print(f\"  Test data: {len(test_df)} rows\")\n",
    "print(f\"  Model: {agent_info['model_name']}\")\n",
    "print(f\"  Device: {agent_info['device']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: ‡∏™‡∏£‡πâ‡∏≤‡∏á Callbacks ‡πÅ‡∏•‡∏∞ Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_callbacks(val_env, model_name):\n",
    "    print(\"üîß Creating training callbacks...\")\n",
    "    eval_callback = EvalCallback(\n",
    "        val_env,\n",
    "        best_model_save_path=os.path.join(MODEL_DIR, f\"best_{model_name.lower()}_model\"),\n",
    "        log_path=os.path.join(LOGS_DIR, f\"eval_{model_name.lower()}\"),\n",
    "        eval_freq=5000,\n",
    "        n_eval_episodes=5,\n",
    "        deterministic=True,\n",
    "        render=False,\n",
    "        verbose=1\n",
    "    )\n",
    "    reward_threshold_callback = StopTrainingOnRewardThreshold(\n",
    "        reward_threshold=1000,\n",
    "        verbose=1\n",
    "    )\n",
    "    callbacks = [eval_callback, reward_threshold_callback]\n",
    "    print(\"‚úÖ Training callbacks created\")\n",
    "    return callbacks\n",
    "\n",
    "def setup_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"‚úÖ Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"‚ÑπÔ∏è Using CPU\")\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" if torch.cuda.is_available() else \"-1\"\n",
    "    return device\n",
    "\n",
    "device = setup_device()\n",
    "for model_name in ['PPO', 'A2C', 'DDPG', 'SAC']:\n",
    "    if model_name in agent_configs:\n",
    "        agent_configs[model_name]['device'] = device\n",
    "model_name = agent_info['model_name']\n",
    "callbacks = create_training_callbacks(val_env, model_name)\n",
    "print(f\"\\nüîß Training setup completed for {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏ó‡∏£‡∏ô Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏ó‡∏£‡∏ô Agent\n",
    "def create_and_train_agent(train_env, model_name, agent_configs, callbacks):\n",
    "    print(f\"ü§ñ Creating and training {model_name} agent...\")\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        agent = DRLAgent(env=train_env)\n",
    "        model_params = agent_configs[model_name].copy()\n",
    "        training_config = agent_configs['TRAINING']\n",
    "        print(f\"üß† Model parameters:\")\n",
    "        for key, value in model_params.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        print(f\"\\n‚è≥ Training configuration:\")\n",
    "        for key, value in training_config.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        model = agent.get_model(model_name.lower(), model_kwargs=model_params)\n",
    "        print(f\"‚úÖ {model_name} model created successfully\")\n",
    "        print(f\"\\nüèÉ Starting training...\")\n",
    "        print(f\"üìä Training timesteps: {training_config['total_timesteps']:,}\")\n",
    "        trained_model = agent.train_model(\n",
    "            model=model,\n",
    "            tb_log_name=training_config['tb_log_name'],\n",
    "            total_timesteps=training_config['total_timesteps'],\n",
    "            callback=callbacks\n",
    "        )\n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"\\n‚úÖ Training completed successfully!\")\n",
    "        print(f\"‚è±Ô∏è Training time: {training_time/60:.2f} minutes\")\n",
    "        model_path = os.path.join(MODEL_DIR, f\"trained_{model_name.lower()}_model\")\n",
    "        trained_model.save(model_path)\n",
    "        print(f\"üíæ Model saved to {model_path}\")\n",
    "        training_info = {\n",
    "            'model_name': model_name,\n",
    "            'training_time_minutes': training_time/60,\n",
    "            'total_timesteps': training_config['total_timesteps'],\n",
    "            'training_start': datetime.now().isoformat(),\n",
    "            'model_params': model_params,\n",
    "            'training_config': training_config,\n",
    "            'model_path': model_path\n",
    "        }\n",
    "        training_info_file = os.path.join(MODEL_DIR, f\"training_info_{model_name.lower()}.pkl\")\n",
    "        with open(training_info_file, 'wb') as f:\n",
    "            pickle.dump(training_info, f)\n",
    "        print(f\"üíæ Training info saved to {training_info_file}\")\n",
    "        return trained_model, training_info\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during training: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "trained_model, training_info = create_and_train_agent(train_env, model_name, agent_configs, callbacks)\n",
    "print(f\"\\nüéâ {model_name} agent training completed!\")\n",
    "print(f\"üìä Training summary:\")\n",
    "print(f\"  Model: {training_info['model_name']}\")\n",
    "print(f\"  Training time: {training_info['training_time_minutes']:.2f} minutes\")\n",
    "print(f\"  Total timesteps: {training_info['total_timesteps']:,}\")\n",
    "print(f\"  Model saved to: {training_info['model_path']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• Training ‡πÅ‡∏•‡∏∞ Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Learning Progress\n",
    "def evaluate_trained_model(trained_model, val_env, test_env):\n",
    "    print(\"üìä Evaluating trained model...\")\n",
    "    results = {}\n",
    "    # Validation\n",
    "    print(\"\\nüîç Validation evaluation...\")\n",
    "    try:\n",
    "        val_account_value, val_actions = DRLAgent.DRL_prediction(\n",
    "            model=trained_model,\n",
    "            environment=val_env\n",
    "        )\n",
    "        val_initial = INITIAL_AMOUNT\n",
    "        val_final = val_account_value['account_value'].iloc[-1]\n",
    "        val_return = (val_final - val_initial) / val_initial * 100\n",
    "        results['validation'] = {\n",
    "            'initial_value': val_initial,\n",
    "            'final_value': val_final,\n",
    "            'total_return': val_return,\n",
    "            'account_values': val_account_value,\n",
    "            'actions': val_actions\n",
    "        }\n",
    "        print(f\"‚úÖ Validation completed\")\n",
    "        print(f\"üí∞ Initial: ${val_initial:,.2f}\")\n",
    "        print(f\"üí∞ Final: ${val_final:,.2f}\")\n",
    "        print(f\"üìà Return: {val_return:.2f}%\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Validation evaluation failed: {str(e)}\")\n",
    "        results['validation'] = None\n",
    "    # Test (preview)\n",
    "    print(\"\\nüîç Test evaluation (preview)...\")\n",
    "    try:\n",
    "        test_account_value, test_actions = DRLAgent.DRL_prediction(\n",
    "            model=trained_model,\n",
    "            environment=test_env\n",
    "        )\n",
    "        test_initial = INITIAL_AMOUNT\n",
    "        test_final = test_account_value['account_value'].iloc[-1]\n",
    "        test_return = (test_final - test_initial) / test_initial * 100\n",
    "        results['test'] = {\n",
    "            'initial_value': test_initial,\n",
    "            'final_value': test_final,\n",
    "            'total_return': test_return,\n",
    "            'account_values': test_account_value,\n",
    "            'actions': test_actions\n",
    "        }\n",
    "        print(f\"‚úÖ Test evaluation completed\")\n",
    "        print(f\"üí∞ Initial: ${test_initial:,.2f}\")\n",
    "        print(f\"üí∞ Final: ${test_final:,.2f}\")\n",
    "        print(f\"üìà Return: {test_return:.2f}%\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Test evaluation failed: {str(e)}\")\n",
    "        results['test'] = None\n",
    "    return results\n",
    "\n",
    "def plot_training_progress(results, model_name):\n",
    "    print(\"üìä Creating training progress plots...\")\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    # Plot 1: Validation Portfolio Value\n",
    "    if results['validation'] is not None:\n",
    "        val_data = results['validation']['account_values']\n",
    "        axes[0, 0].plot(val_data['account_value'], color='blue', linewidth=2)\n",
    "        axes[0, 0].axhline(y=INITIAL_AMOUNT, color='red', linestyle='--', alpha=0.7, label='Initial Value')\n",
    "        axes[0, 0].set_title('Validation Portfolio Value')\n",
    "        axes[0, 0].set_ylabel('Portfolio Value ($)')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[0, 0].text(0.5, 0.5, 'Validation Data\\nNot Available', ha='center', va='center', transform=axes[0, 0].transAxes)\n",
    "    # Plot 2: Test Portfolio Value\n",
    "    if results['test'] is not None:\n",
    "        test_data = results['test']['account_values']\n",
    "        axes[0, 1].plot(test_data['account_value'], color='green', linewidth=2)\n",
    "        axes[0, 1].axhline(y=INITIAL_AMOUNT, color='red', linestyle='--', alpha=0.7, label='Initial Value')\n",
    "        axes[0, 1].set_title('Test Portfolio Value')\n",
    "        axes[0, 1].set_ylabel('Portfolio Value ($)')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[0, 1].text(0.5, 0.5, 'Test Data\\nNot Available', ha='center', va='center', transform=axes[0, 1].transAxes)\n",
    "    # Plot 3: Returns Comparison\n",
    "    returns_data = []\n",
    "    labels = []\n",
    "    colors = []\n",
    "    if results['validation'] is not None:\n",
    "        returns_data.append(results['validation']['total_return'])\n",
    "        labels.append('Validation')\n",
    "        colors.append('blue')\n",
    "    if results['test'] is not None:\n",
    "        returns_data.append(results['test']['total_return'])\n",
    "        labels.append('Test')\n",
    "        colors.append('green')\n",
    "    if returns_data:\n",
    "        bars = axes[1, 0].bar(labels, returns_data, color=colors, alpha=0.7)\n",
    "        axes[1, 0].set_title('Returns Comparison')\n",
    "        axes[1, 0].set_ylabel('Return (%)')\n",
    "        axes[1, 0].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "        for bar, value in zip(bars, returns_data):\n",
    "            height = bar.get_height()\n",
    "            axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + (0.5 if height > 0 else -1.5), f'{value:.2f}%', ha='center', va='bottom' if height > 0 else 'top')\n",
    "    else:\n",
    "        axes[1, 0].text(0.5, 0.5, 'No Return Data\\nAvailable', ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "    # Plot 4: Training Summary\n",
    "    axes[1, 1].axis('off')\n",
    "    summary_text = f\"Model: {model_name}\\n\"\n",
    "    axes[1, 1].text(0.1, 0.5, summary_text, fontsize=14, va='center')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Learning Progress\n",
    "results = evaluate_trained_model(trained_model, val_env, test_env)\n",
    "fig = plot_training_progress(results, model_name)\n",
    "print(\"\\n‚úÖ Training evaluation and analysis completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}