{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Data Preprocessing)\n",
    "## ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Crypto Trading\n",
    "\n",
    "### ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢:\n",
    "- ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "- ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Technical Indicators\n",
    "- Normalize ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "- ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô Training/Validation/Testing\n",
    "- ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Import Libraries ‡πÅ‡∏•‡∏∞‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import ta  # Technical Analysis library\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Import config\n",
    "from config import *\n",
    "\n",
    "# Setup directories\n",
    "RAW_DIR = \"raw_data\"\n",
    "PROCESSED_DIR = \"processed_data\"\n",
    "\n",
    "if not os.path.exists(PROCESSED_DIR):\n",
    "    os.makedirs(PROCESSED_DIR)\n",
    "\n",
    "print(\"üìÅ Setup directories completed\")\n",
    "print(f\"üìä Starting Data Preprocessing Process\")\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_data():\n",
    "    \"\"\"\n",
    "    ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV\n",
    "    \"\"\"\n",
    "    print(\"üìÇ Loading raw data...\")\n",
    "    \n",
    "    # ‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡πÉ‡∏ô raw_data\n",
    "    csv_files = [f for f in os.listdir(RAW_DIR) if f.endswith('.csv')]\n",
    "    if not csv_files:\n",
    "        raise FileNotFoundError(\"No CSV files found in raw_data directory\")\n",
    "    \n",
    "    latest_file = max(csv_files, key=lambda x: os.path.getctime(os.path.join(RAW_DIR, x)))\n",
    "    file_path = os.path.join(RAW_DIR, latest_file)\n",
    "    \n",
    "    # ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(df)} rows from {latest_file}\")\n",
    "    print(f\"üìÖ Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "    print(f\"üìä Symbols: {df['tic'].unique()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "    \"\"\"\n",
    "    print(\"üßπ Cleaning data...\")\n",
    "    \n",
    "    # ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡πâ‡∏≥\n",
    "    initial_len = len(df)\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"  Removed {initial_len - len(df)} duplicate rows\")\n",
    "    \n",
    "    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤\n",
    "    df = df.sort_values(['timestamp', 'tic'])\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    if missing_values.any():\n",
    "        print(\"\\n‚ö†Ô∏è Found missing values:\")\n",
    "        print(missing_values[missing_values > 0])\n",
    "        \n",
    "        # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç missing values\n",
    "        for col in df.columns:\n",
    "            if df[col].isnull().any():\n",
    "                if col in ['open', 'high', 'low', 'close']:\n",
    "                    # Forward fill ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏≤‡∏Ñ‡∏≤\n",
    "                    df[col] = df.groupby('tic')[col].fillna(method='ffill')\n",
    "                elif col == 'volume':\n",
    "                    # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏î‡πâ‡∏ß‡∏¢ 0 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö volume\n",
    "                    df[col] = df[col].fillna(0)\n",
    "    \n",
    "    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡πà‡∏≤‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥\n",
    "    numeric_columns = ['open', 'high', 'low', 'close', 'volume']\n",
    "    for col in numeric_columns:\n",
    "        q1 = df[col].quantile(0.25)\n",
    "        q3 = df[col].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "        \n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "        if len(outliers) > 0:\n",
    "            print(f\"\\n‚ö†Ô∏è Found {len(outliers)} outliers in {col}\")\n",
    "            # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡πà‡∏≤‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á\n",
    "            df.loc[df[col] < lower_bound, col] = lower_bound\n",
    "            df.loc[df[col] > upper_bound, col] = upper_bound\n",
    "    \n",
    "    print(\"‚úÖ Data cleaning completed\")\n",
    "    return df\n",
    "\n",
    "# ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "df = load_raw_data()\n",
    "df = clean_data(df)\n",
    "\n",
    "print(f\"\\nüìä Cleaned data summary:\")\n",
    "print(f\"  Total rows: {len(df)}\")\n",
    "print(f\"  Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "print(f\"  Symbols: {df['tic'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Technical Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_technical_indicators(df):\n",
    "    \"\"\"\n",
    "    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Technical Indicators ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ symbol\n",
    "    \"\"\"\n",
    "    print(\"üìà Calculating technical indicators...\")\n",
    "    \n",
    "    # ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡πÉ‡∏´‡∏°‡πà\n",
    "    processed_df = pd.DataFrame()\n",
    "    \n",
    "    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì indicators ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ symbol\n",
    "    for symbol in df['tic'].unique():\n",
    "        print(f\"\\nProcessing {symbol}...\")\n",
    "        symbol_data = df[df['tic'] == symbol].copy()\n",
    "        \n",
    "        # Trend Indicators\n",
    "        symbol_data['sma_20'] = ta.trend.sma_indicator(symbol_data['close'], window=20)\n",
    "        symbol_data['sma_50'] = ta.trend.sma_indicator(symbol_data['close'], window=50)\n",
    "        symbol_data['ema_20'] = ta.trend.ema_indicator(symbol_data['close'], window=20)\n",
    "        symbol_data['macd'] = ta.trend.macd_diff(symbol_data['close'])\n",
    "        \n",
    "        # Momentum Indicators\n",
    "        symbol_data['rsi'] = ta.momentum.rsi(symbol_data['close'], window=14)\n",
    "        symbol_data['stoch'] = ta.momentum.stoch(symbol_data['high'], symbol_data['low'], symbol_data['close'])\n",
    "        symbol_data['williams_r'] = ta.momentum.williams_r(symbol_data['high'], symbol_data['low'], symbol_data['close'])\n",
    "        \n",
    "        # Volatility Indicators\n",
    "        symbol_data['bb_high'] = ta.volatility.bollinger_hband(symbol_data['close'])\n",
    "        symbol_data['bb_low'] = ta.volatility.bollinger_lband(symbol_data['close'])\n",
    "        symbol_data['atr'] = ta.volatility.average_true_range(symbol_data['high'], symbol_data['low'], symbol_data['close'])\n",
    "        \n",
    "        # Volume Indicators\n",
    "        symbol_data['obv'] = ta.volume.on_balance_volume(symbol_data['close'], symbol_data['volume'])\n",
    "        symbol_data['vwap'] = ta.volume.volume_weighted_average_price(symbol_data['high'], symbol_data['low'], symbol_data['close'], symbol_data['volume'])\n",
    "        \n",
    "        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÅ‡∏•‡πâ‡∏ß\n",
    "        processed_df = pd.concat([processed_df, symbol_data])\n",
    "        \n",
    "        print(f\"‚úÖ Added {len(symbol_data)} rows with indicators\")\n",
    "    \n",
    "    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "    processed_df = processed_df.sort_values(['timestamp', 'tic']).reset_index(drop=True)\n",
    "    \n",
    "    print(\"\\n‚úÖ Technical indicators calculation completed\")\n",
    "    print(f\"üìä Total indicators: {len(processed_df.columns) - 6}\")  # ‡∏•‡∏ö 6 ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô\n",
    "    \n",
    "    return processed_df\n",
    "\n",
    "# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì technical indicators\n",
    "df = calculate_technical_indicators(df)\n",
    "\n",
    "print(f\"\\nüìä Indicators summary:\")\n",
    "print(f\"  Total columns: {len(df.columns)}\")\n",
    "print(f\"  Indicators: {[col for col in df.columns if col not in ['timestamp', 'tic', 'open', 'high', 'low', 'close', 'volume']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Normalize ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(df):\n",
    "    \"\"\"\n",
    "    Normalize ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏≤‡∏Ñ‡∏≤‡πÅ‡∏•‡∏∞ indicators\n",
    "    \"\"\"\n",
    "    print(\"üìä Normalizing data...\")\n",
    "    \n",
    "    # ‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á normalize\n",
    "    non_numeric_cols = ['timestamp', 'tic']\n",
    "    numeric_cols = [col for col in df.columns if col not in non_numeric_cols]\n",
    "    \n",
    "    # ‡∏™‡∏£‡πâ‡∏≤‡∏á scaler\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    # Normalize ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ symbol\n",
    "    normalized_data = pd.DataFrame()\n",
    "    \n",
    "    for symbol in df['tic'].unique():\n",
    "        print(f\"\\nNormalizing {symbol}...\")\n",
    "        symbol_data = df[df['tic'] == symbol].copy()\n",
    "        \n",
    "        # Normalize ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç\n",
    "        numeric_data = symbol_data[numeric_cols]\n",
    "        normalized_numeric = scaler.fit_transform(numeric_data)\n",
    "        \n",
    "        # ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡πÉ‡∏´‡∏°‡πà\n",
    "        normalized_symbol_data = pd.DataFrame(normalized_numeric, columns=numeric_cols)\n",
    "        normalized_symbol_data[non_numeric_cols] = symbol_data[non_numeric_cols]\n",
    "        \n",
    "        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà normalize ‡πÅ‡∏•‡πâ‡∏ß\n",
    "        normalized_data = pd.concat([normalized_data, normalized_symbol_data])\n",
    "        \n",
    "        print(f\"‚úÖ Normalized {len(symbol_data)} rows\")\n",
    "    \n",
    "    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "    normalized_data = normalized_data.sort_values(['timestamp', 'tic']).reset_index(drop=True)\n",
    "    \n",
    "    print(\"\\n‚úÖ Data normalization completed\")\n",
    "    return normalized_data\n",
    "\n",
    "# Normalize ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "df = normalize_data(df)\n",
    "\n",
    "print(f\"\\nüìä Normalized data summary:\")\n",
    "print(f\"  Total rows: {len(df)}\")\n",
    "print(f\"  Total columns: {len(df.columns)}\")\n",
    "print(f\"  Numeric columns: {[col for col in df.columns if col not in ['timestamp', 'tic']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df):\n",
    "    \"\"\"\n",
    "    ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô Training/Validation/Testing\n",
    "    \"\"\"\n",
    "    print(\"üìä Splitting data...\")\n",
    "    \n",
    "    # ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏°‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô 70:15:15\n",
    "    total_len = len(df)\n",
    "    train_size = int(total_len * 0.7)\n",
    "    val_size = int(total_len * 0.15)\n",
    "    \n",
    "    train_df = df.iloc[:train_size].reset_index(drop=True)\n",
    "    val_df = df.iloc[train_size:train_size + val_size].reset_index(drop=True)\n",
    "    test_df = df.iloc[train_size + val_size:].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"‚úÖ Data split completed:\")\n",
    "    print(f\"  Training set: {len(train_df)} rows ({len(train_df)/total_len*100:.1f}%)\")\n",
    "    print(f\"  Validation set: {len(val_df)} rows ({len(val_df)/total_len*100:.1f}%)\")\n",
    "    print(f\"  Testing set: {len(test_df)} rows ({len(test_df)/total_len*100:.1f}%)\")\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "def save_processed_data(df, train_df, val_df, test_df):\n",
    "    \"\"\"\n",
    "    ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß\n",
    "    \"\"\"\n",
    "    print(\"üíæ Saving processed data...\")\n",
    "    \n",
    "    # ‡∏™‡∏£‡πâ‡∏≤‡∏á timestamp\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
    "    pickle_file = os.path.join(PROCESSED_DIR, f\"processed_crypto_data_{timestamp}.pkl\")\n",
    "    with open(pickle_file, 'wb') as f:\n",
    "        pickle.dump(df, f)\n",
    "    print(f\"‚úÖ Saved full data to {pickle_file}\")\n",
    "    \n",
    "    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏¢‡∏Å‡∏™‡πà‡∏ß‡∏ô\n",
    "    train_file = os.path.join(PROCESSED_DIR, f\"train_data_{timestamp}.pkl\")\n",
    "    val_file = os.path.join(PROCESSED_DIR, f\"val_data_{timestamp}.pkl\")\n",
    "    test_file = os.path.join(PROCESSED_DIR, f\"test_data_{timestamp}.pkl\")\n",
    "    \n",
    "    with open(train_file, 'wb') as f:\n",
    "        pickle.dump(train_df, f)\n",
    "    with open(val_file, 'wb') as f:\n",
    "        pickle.dump(val_df, f)\n",
    "    with open(test_file, 'wb') as f:\n",
    "        pickle.dump(test_df, f)\n",
    "    \n",
    "    print(f\"‚úÖ Saved split data to:\")\n",
    "    print(f\"  - {train_file}\")\n",
    "    print(f\"  - {val_file}\")\n",
    "    print(f\"  - {test_file}\")\n",
    "    \n",
    "    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î\n",
    "    latest_files = {\n",
    "        'full': os.path.join(PROCESSED_DIR, \"processed_crypto_data.pkl\"),\n",
    "        'train': os.path.join(PROCESSED_DIR, \"train_data.pkl\"),\n",
    "        'val': os.path.join(PROCESSED_DIR, \"val_data.pkl\"),\n",
    "        'test': os.path.join(PROCESSED_DIR, \"test_data.pkl\")\n",
    "    }\n",
    "    \n",
    "    for key, file_path in latest_files.items():\n",
    "        if os.path.exists(file_path):\n",
    "            os.remove(file_path)\n",
    "        \n",
    "        if key == 'full':\n",
    "            df.to_csv(file_path.replace('.pkl', '.csv'), index=False)\n",
    "        else:\n",
    "            eval(f\"{key}_df\").to_csv(file_path.replace('.pkl', '.csv'), index=False)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Saved latest files to:\")\n",
    "    for key, file_path in latest_files.items():\n",
    "        print(f\"  - {file_path.replace('.pkl', '.csv')}\")\n",
    "\n",
    "# ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "train_df, val_df, test_df = split_data(df)\n",
    "\n",
    "# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "save_processed_data(df, train_df, val_df, test_df)\n",
    "\n",
    "print(f\"\\n   Data preprocessing completed successfully!\")\n",
    "print(f\"üìä Final data summary:\")\n",
    "print(f\"  Total rows: {len(df)}\")\n",
    "print(f\"  Total columns: {len(df.columns)}\")\n",
    "print(f\"  Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "print(f\"  Symbols: {df['tic'].unique()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}