{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# 2. ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Agent (Agent Creation)\n",
    "## ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á RL Agent ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Crypto Trading\n",
    "\n",
    "### ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢:\n",
    "- ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß\n",
    "- ‡∏™‡∏£‡πâ‡∏≤‡∏á Trading Environment\n",
    "- ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Agent Architecture\n",
    "- ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Hyperparameters\n",
    "- ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Agent ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Cell 1: Import Libraries ‡πÅ‡∏•‡∏∞ Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "# FinRL imports\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "\n",
    "# Import config\n",
    "from config import *\n",
    "\n",
    "# Setup directories\n",
    "PROCESSED_DIR = \"processed_data\"\n",
    "AGENT_DIR = \"agents\"\n",
    "for dir_name in [AGENT_DIR]:\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "\n",
    "print(\"üìÅ Setup directories completed\")\n",
    "print(f\"ü§ñ Starting Agent Creation Process\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Cell 2: ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß\n",
    "def load_processed_data():\n",
    "    print(\"üìÇ Loading processed data...\")\n",
    "    try:\n",
    "        # ‡∏•‡∏≠‡∏á‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå pickle ‡∏Å‡πà‡∏≠‡∏ô (‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤)\n",
    "        pickle_file = os.path.join(PROCESSED_DIR, \"processed_crypto_data.pkl\")\n",
    "        with open(pickle_file, 'rb') as f:\n",
    "            df = pickle.load(f)\n",
    "        print(f\"‚úÖ Loaded processed data from {pickle_file}\")\n",
    "    except:\n",
    "        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå pickle ‡πÉ‡∏´‡πâ‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å CSV\n",
    "        csv_file = os.path.join(PROCESSED_DIR, \"processed_crypto_data.csv\")\n",
    "        df = pd.read_csv(csv_file)\n",
    "        # ‡πÅ‡∏õ‡∏•‡∏á timestamp ‡πÄ‡∏õ‡πá‡∏ô datetime\n",
    "        if 'timestamp' in df.columns:\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        elif 'date' in df.columns:\n",
    "            df['timestamp'] = pd.to_datetime(df['date'])\n",
    "        print(f\"‚úÖ Loaded processed data from {csv_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "df = load_processed_data()\n",
    "print(f\"üìä Data shape: {df.shape}\")\n",
    "print(f\"üóìÔ∏è Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "print(f\"üí∞ Cryptocurrencies: {sorted(df['tic'].unique())}\")\n",
    "\n",
    "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö columns ‡∏ó‡∏µ‡πà‡∏°‡∏µ\n",
    "print(f\"\\nüìã Available columns:\")\n",
    "for i, col in enumerate(df.columns):\n",
    "    print(f\"  {i+1:2d}. {col}\")\n",
    "\n",
    "# ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "print(f\"\\nüìà Sample data:\")\n",
    "display_cols = ['timestamp', 'tic', 'Close', 'Volume']\n",
    "if 'sma_20' in df.columns:\n",
    "    display_cols.extend(['sma_20', 'rsi', 'macd'])\n",
    "print(df[display_cols].head())\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Cell 3: ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á Trading Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô Train/Validation/Test\n",
    "def split_data(df, train_ratio=0.7, val_ratio=0.15):\n",
    "    \"\"\"\n",
    "    ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô 3 ‡∏™‡πà‡∏ß‡∏ô:\n",
    "    - Training: 70%\n",
    "    - Validation: 15% \n",
    "    - Test: 15%\n",
    "    \"\"\"\n",
    "    print(\"üìä Splitting data...\")\n",
    "    \n",
    "    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏°‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡∏∞ cryptocurrency\n",
    "    df = df.copy()\n",
    "    df['date'] = df['timestamp'].dt.date\n",
    "    df.sort_values(['date', 'tic'], inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏™‡πà‡∏ß‡∏ô\n",
    "    total_len = len(df)\n",
    "    train_size = int(total_len * train_ratio)\n",
    "    val_size = int(total_len * val_ratio)\n",
    "    \n",
    "    # ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "    train_df = df.iloc[:train_size].reset_index(drop=True)\n",
    "    val_df = df.iloc[train_size:train_size + val_size].reset_index(drop=True)\n",
    "    test_df = df.iloc[train_size + val_size:].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"‚úÖ Data split completed:\")\n",
    "    print(f\"  üìà Train: {len(train_df):,} rows ({len(train_df)/total_len*100:.1f}%)\")\n",
    "    print(f\"  üìä Validation: {len(val_df):,} rows ({len(val_df)/total_len*100:.1f}%)\")\n",
    "    print(f\"  üìâ Test: {len(test_df):,} rows ({len(test_df)/total_len*100:.1f}%)\")\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á Trading Environment\n",
    "def create_trading_environment(df, initial_amount=INITIAL_AMOUNT, \n",
    "                              transaction_cost_pct=TRANSACTION_COST_PCT):\n",
    "    \"\"\"\n",
    "    ‡∏™‡∏£‡πâ‡∏≤‡∏á FinRL Trading Environment\n",
    "    \"\"\"\n",
    "    print(f\"üèõÔ∏è Creating trading environment...\")\n",
    "    \n",
    "    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö environment\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ feature columns ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô\n",
    "    required_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    for col in required_cols:\n",
    "        if col not in df_processed.columns:\n",
    "            raise ValueError(f\"Missing required column: {col}\")\n",
    "    \n",
    "    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° feature columns ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö agent\n",
    "    feature_cols = []\n",
    "    for col in df_processed.columns:\n",
    "        if col not in ['date', 'timestamp', 'tic'] and not col.startswith('Adj'):\n",
    "            feature_cols.append(col)\n",
    "    \n",
    "    print(f\"üìã Feature columns: {feature_cols}\")\n",
    "    \n",
    "    # ‡∏™‡∏£‡πâ‡∏≤‡∏á environment configuration\n",
    "    env_kwargs = {\n",
    "        'df': df_processed,\n",
    "        'stock_dim': len(df_processed['tic'].unique()),\n",
    "        'hmax': 100,  # maximum shares ‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ñ‡∏∑‡∏≠‡πÑ‡∏î‡πâ\n",
    "        'initial_amount': initial_amount,\n",
    "        'transaction_cost_pct': transaction_cost_pct,\n",
    "        'reward_scaling': 1e-4,\n",
    "        'state_space': len(feature_cols),\n",
    "        'action_space': len(df_processed['tic'].unique()),\n",
    "        'tech_indicator_list': feature_cols,\n",
    "        'print_verbosity': 1\n",
    "    }\n",
    "    \n",
    "    # ‡∏™‡∏£‡πâ‡∏≤‡∏á environment\n",
    "    env = StockTradingEnv(**env_kwargs)\n",
    "    \n",
    "    print(f\"‚úÖ Environment created successfully:\")\n",
    "    print(f\"  üí∞ Initial amount: ${initial_amount:,.2f}\")\n",
    "    print(f\"  üí∏ Transaction cost: {transaction_cost_pct*100:.3f}%\")\n",
    "    print(f\"  üìä State space: {env_kwargs['state_space']}\")\n",
    "    print(f\"  üéØ Action space: {env_kwargs['action_space']}\")\n",
    "    print(f\"  üè™ Stock dimension: {env_kwargs['stock_dim']}\")\n",
    "    \n",
    "    return env, env_kwargs\n",
    "\n",
    "# ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "train_df, val_df, test_df = split_data(df)\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á environments\n",
    "train_env, train_env_kwargs = create_trading_environment(train_df)\n",
    "val_env, val_env_kwargs = create_trading_environment(val_df)\n",
    "test_env, test_env_kwargs = create_trading_environment(test_df)\n",
    "\n",
    "print(f\"\\n‚úÖ All environments created successfully!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Cell 4: ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Agent Configurations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î configurations ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RL Agents ‡∏ï‡πà‡∏≤‡∏á‡πÜ\n",
    "def setup_agent_configurations():\n",
    "    \"\"\"\n",
    "    ‡∏Å‡∏≥‡∏´‡∏ô‡∏î hyperparameters ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RL algorithms ‡∏ï‡πà‡∏≤‡∏á‡πÜ\n",
    "    \"\"\"\n",
    "    print(\"üîß Setting up agent configurations...\")\n",
    "    \n",
    "    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö device ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"‚úÖ Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"‚ÑπÔ∏è Using CPU\")\n",
    "    \n",
    "    # Agent configurations\n",
    "    agent_configs = {\n",
    "        # PPO (Proximal Policy Optimization) - ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö continuous action spaces\n",
    "        'PPO': {\n",
    "            'learning_rate': 3e-4,\n",
    "            'n_steps': 2048,\n",
    "            'batch_size': 64,\n",
    "            'n_epochs': 10,\n",
    "            'gamma': 0.99,\n",
    "            'gae_lambda': 0.95,\n",
    "            'clip_range': 0.2,\n",
    "            'ent_coef': 0.01,\n",
    "            'vf_coef': 0.5,\n",
    "            'max_grad_norm': 0.5,\n",
    "            'policy_kwargs': dict(net_arch=[256, 256]),\n",
    "            'device': device,\n",
    "            'verbose': 1\n",
    "        },\n",
    "        \n",
    "        # A2C (Advantage Actor-Critic) - ‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏ï‡πà‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡πÄ‡∏ó‡πà‡∏≤ PPO\n",
    "        'A2C': {\n",
    "            'learning_rate': 7e-4,\n",
    "            'n_steps': 5,\n",
    "            'gamma': 0.99,\n",
    "            'gae_lambda': 1.0,\n",
    "            'ent_coef': 0.01,\n",
    "            'vf_coef': 0.25,\n",
    "            'max_grad_norm': 0.5,\n",
    "            'policy_kwargs': dict(net_arch=[256, 256]),\n",
    "            'device': device,\n",
    "            'verbose': 1\n",
    "        },\n",
    "        \n",
    "        # DDPG (Deep Deterministic Policy Gradient) - ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö continuous actions\n",
    "        'DDPG': {\n",
    "            'learning_rate': 1e-3,\n",
    "            'buffer_size': 1000000,\n",
    "            'learning_starts': 100,\n",
    "            'batch_size': 256,\n",
    "            'tau': 0.005,\n",
    "            'gamma': 0.99,\n",
    "            'policy_kwargs': dict(net_arch=[256, 256]),\n",
    "            'device': device,\n",
    "            'verbose': 1\n",
    "        },\n",
    "        \n",
    "        # SAC (Soft Actor-Critic) - ‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö continuous control\n",
    "        'SAC': {\n",
    "            'learning_rate': 3e-4,\n",
    "            'buffer_size': 1000000,\n",
    "            'learning_starts': 100,\n",
    "            'batch_size': 256,\n",
    "            'tau': 0.005,\n",
    "            'gamma': 0.99,\n",
    "            'ent_coef': 'auto',\n",
    "            'policy_kwargs': dict(net_arch=[256, 256]),\n",
    "            'device': device,\n",
    "            'verbose': 1\n",
    "        },\n",
    "        \n",
    "        # Training configuration\n",
    "        'TRAINING': {\n",
    "            'total_timesteps': 100000,  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô timesteps ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô\n",
    "            'tb_log_name': 'crypto_trading',\n",
    "            'eval_freq': 5000,\n",
    "            'n_eval_episodes': 5,\n",
    "            'save_freq': 10000\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return agent_configs, device\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á agent configurations\n",
    "agent_configs, device = setup_agent_configurations()\n",
    "\n",
    "# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å model ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ (‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏î‡πâ)\n",
    "MODEL_NAME = 'PPO'  # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô 'A2C', 'DDPG', ‡∏´‡∏£‡∏∑‡∏≠ 'SAC' ‡πÑ‡∏î‡πâ\n",
    "\n",
    "print(f\"\\nü§ñ Selected Model: {MODEL_NAME}\")\n",
    "print(f\"üîß Model Configuration:\")\n",
    "for key, value in agent_configs[MODEL_NAME].items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nüìä Training Configuration:\")\n",
    "for key, value in agent_configs['TRAINING'].items():\n",
    "    print(f\"  {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Cell 5: ‡∏™‡∏£‡πâ‡∏≤‡∏á RL Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á RL Agent\n",
    "def create_agent(env, model_name, agent_configs):\n",
    "    \"\"\"\n",
    "    ‡∏™‡∏£‡πâ‡∏≤‡∏á RL Agent ‡∏î‡πâ‡∏ß‡∏¢ FinRL\n",
    "    \"\"\"\n",
    "    print(f\"ü§ñ Creating {model_name} agent...\")\n",
    "    \n",
    "    try:\n",
    "        # ‡∏™‡∏£‡πâ‡∏≤‡∏á DRLAgent\n",
    "        agent = DRLAgent(env=env)\n",
    "        \n",
    "        # ‡∏î‡∏∂‡∏á model parameters\n",
    "        model_params = agent_configs[model_name].copy()\n",
    "        \n",
    "        # ‡∏™‡∏£‡πâ‡∏≤‡∏á model\n",
    "        model = agent.get_model(model_name.lower(), model_kwargs=model_params)\n",
    "        \n",
    "        print(f\"‚úÖ {model_name} agent created successfully!\")\n",
    "        print(f\"üìã Model summary:\")\n",
    "        print(f\"  Algorithm: {model_name}\")\n",
    "        print(f\"  Policy: {type(model.policy).__name__}\")\n",
    "        print(f\"  Device: {model.device}\")\n",
    "        \n",
    "        return agent, model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating {model_name} agent: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á agent\n",
    "agent, model = create_agent(train_env, MODEL_NAME, agent_configs)\n",
    "\n",
    "if model is not None:\n",
    "    print(f\"\\nüéØ Agent ready for training!\")\n",
    "    print(f\"üìä Environment observation space: {train_env.observation_space}\")\n",
    "    print(f\"üéÆ Environment action space: {train_env.action_space}\")\n",
    "else:\n",
    "    print(f\"‚ùå Failed to create agent. Please check configurations.\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Cell 6: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Agent ‡πÅ‡∏•‡∏∞ Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ\n",
    "def save_agent_setup(df, train_env_kwargs, agent_configs, model_name, device):\n",
    "    \"\"\"\n",
    "    ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• setup ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ\n",
    "    \"\"\"\n",
    "    print(\"üíæ Saving agent setup...\")\n",
    "    \n",
    "    try:\n",
    "        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß\n",
    "        with open(os.path.join(PROCESSED_DIR, \"processed_crypto_data.pkl\"), 'wb') as f:\n",
    "            pickle.dump(df, f)\n",
    "        print(\"‚úÖ Saved processed data as pickle file\")\n",
    "        \n",
    "        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å environment configuration\n",
    "        env_config = {\n",
    "            'env_kwargs': train_env_kwargs,\n",
    "            'model_name': model_name,\n",
    "            'device': str(device)\n",
    "        }\n",
    "        with open(os.path.join(AGENT_DIR, \"environment_config.pkl\"), 'wb') as f:\n",
    "            pickle.dump(env_config, f)\n",
    "        print(\"‚úÖ Saved environment configuration\")\n",
    "        \n",
    "        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å agent configurations\n",
    "        with open(os.path.join(AGENT_DIR, \"agent_configs.pkl\"), 'wb') as f:\n",
    "            pickle.dump(agent_configs, f)\n",
    "        print(\"‚úÖ Saved agent configurations\")\n",
    "        \n",
    "        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• agent info\n",
    "        agent_info = {\n",
    "            'model_name': model_name,\n",
    "            'device': str(device),\n",
    "            'creation_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'data_shape': df.shape,\n",
    "            'cryptocurrencies': sorted(df['tic'].unique().tolist()),\n",
    "            'feature_columns': train_env_kwargs['tech_indicator_list']\n",
    "        }\n",
    "        with open(os.path.join(AGENT_DIR, \"agent_info.pkl\"), 'wb') as f:\n",
    "            pickle.dump(agent_info, f)\n",
    "        print(\"‚úÖ Saved agent information\")\n",
    "        \n",
    "        print(f\"\\nüìã Agent Setup Summary:\")\n",
    "        print(f\"  ü§ñ Model: {model_name}\")\n",
    "        print(f\"  üíª Device: {device}\")\n",
    "        print(f\"  üìä Data shape: {df.shape}\")\n",
    "        print(f\"  üí∞ Cryptocurrencies: {len(df['tic'].unique())}\")\n",
    "        print(f\"  üîß Features: {len(train_env_kwargs['tech_indicator_list'])}\")\n",
    "        print(f\"  üí∞ Initial amount: ${train_env_kwargs['initial_amount']:,.2f}\")\n",
    "        print(f\"  üí∏ Transaction cost: {train_env_kwargs['transaction_cost_pct']*100:.3f}%\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving agent setup: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö environment\n",
    "def test_environment(env, steps=5):\n",
    "    \"\"\"\n",
    "    ‡∏ó‡∏î‡∏™‡∏≠‡∏ö environment ‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏õ‡∏Å‡∏ï‡∏¥\n",
    "    \"\"\"\n",
    "    print(f\"üß™ Testing environment...\")\n",
    "    \n",
    "    try:\n",
    "        obs = env.reset()\n",
    "        print(f\"‚úÖ Environment reset successful\")\n",
    "        print(f\"üìä Initial observation shape: {obs.shape}\")\n",
    "        \n",
    "        total_reward = 0\n",
    "        for step in range(steps):\n",
    "            # ‡∏™‡∏∏‡πà‡∏° action\n",
    "            action = env.action_space.sample()\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                print(f\"üèÅ Episode finished at step {step+1}\")\n",
    "                break\n",
    "                \n",
    "        print(f\"‚úÖ Environment test completed\")\n",
    "        print(f\"üéØ Total reward from {steps} random steps: {total_reward:.4f}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Environment test failed: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏ó‡∏î‡∏™‡∏≠‡∏ö\n",
    "if model is not None:\n",
    "    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "    save_success = save_agent_setup(df, train_env_kwargs, agent_configs, MODEL_NAME, device)\n",
    "    \n",
    "    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö environment\n",
    "    test_success = test_environment(train_env)\n",
    "    \n",
    "    if save_success and test_success:\n",
    "        print(f\"\\nüéâ Agent creation completed successfully!\")\n",
    "        print(f\"üìÇ All files saved in: {AGENT_DIR}\")\n",
    "        print(f\"üöÄ Ready to proceed to agent training!\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è Some issues occurred during setup. Please check the logs.\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Agent creation failed. Cannot proceed to saving.\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## ‡∏™‡∏£‡∏∏‡∏õ\n",
    "\n",
    "‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ:\n",
    "\n",
    "1. **‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**: ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• cryptocurrency ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏≤‡∏Å‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤\n",
    "2. **‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**: ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô Training (70%), Validation (15%), ‡πÅ‡∏•‡∏∞ Test (15%)\n",
    "3. **‡∏™‡∏£‡πâ‡∏≤‡∏á Environment**: ‡∏™‡∏£‡πâ‡∏≤‡∏á Trading Environment ‡∏î‡πâ‡∏ß‡∏¢ FinRL ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "4. **‡∏Å‡∏≥‡∏´‡∏ô‡∏î Agent Config**: ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ hyperparameters ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RL algorithms (PPO, A2C, DDPG, SAC)\n",
    "5. **‡∏™‡∏£‡πâ‡∏≤‡∏á Agent**: ‡∏™‡∏£‡πâ‡∏≤‡∏á RL Agent ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\n",
    "6. **‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ\n",
    "\n",
    "**‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ**: ‡πÑ‡∏õ‡∏ó‡∏µ‡πà `3_agent_training.ipynb` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ó‡∏£‡∏ô Agent ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ß‡πâ\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
