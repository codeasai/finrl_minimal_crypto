# FinRL Models ‡πÅ‡∏•‡∏∞ Algorithms Guide

> ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Deep Reinforcement Learning Models ‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÉ‡∏ô FinRL Framework

## üìã ‡∏™‡∏≤‡∏£‡∏ö‡∏±‡∏ç

1. [‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°](#‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°)
2. [On-Policy Algorithms](#on-policy-algorithms)
3. [Off-Policy Algorithms](#off-policy-algorithms)
4. [High-Throughput Algorithms](#high-throughput-algorithms)
5. [Model-based RL](#model-based-rl)
6. [Offline RL ‡πÅ‡∏•‡∏∞ Imitation Learning](#offline-rl-‡πÅ‡∏•‡∏∞-imitation-learning)
7. [Algorithm Extensions](#algorithm-extensions)
8. [‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Algorithm](#‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å-algorithm)
9. [‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÉ‡∏ô finrl_minimal_crypto](#‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÉ‡∏ô-finrl_minimal_crypto)
10. [‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô](#‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô)

---

## üéØ ‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°

FinRL ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Deep Reinforcement Learning algorithms ‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢ ‡∏ã‡∏∂‡πà‡∏á‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏ö‡πà‡∏á‡πÑ‡∏î‡πâ‡πÄ‡∏õ‡πá‡∏ô 6 ‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏•‡∏±‡∏Å:

| ‡∏´‡∏°‡∏ß‡∏î | ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Algorithms | ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö |
|------|------------------|-------------|
| **On-Policy** | 1 | Stable training, good sample efficiency |
| **Off-Policy** | 3 | Sample efficiency, continuous actions |
| **High-Throughput** | 2 | Distributed training, scalability |
| **Model-based** | 1 | Sample efficiency, complex environments |
| **Offline RL** | 3 | Historical data, imitation learning |
| **Extensions** | 1+ | Exploration, curiosity-driven learning |

---

## üöÄ On-Policy Algorithms

### PPO (Proximal Policy Optimization) ‚≠ê **‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥**

**‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:**
- Policy gradient algorithm ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ clipping ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏° policy updates
- ‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á sample efficiency ‡πÅ‡∏•‡∏∞ stability
- ‡πÄ‡∏õ‡πá‡∏ô default algorithm ‡πÉ‡∏ô finrl_minimal_crypto

**‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô:**
- ‚úÖ Stable training
- ‚úÖ ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á continuous ‡πÅ‡∏•‡∏∞ discrete actions
- ‚úÖ Good performance across various tasks
- ‚úÖ Easy to tune hyperparameters

**‡∏à‡∏∏‡∏î‡∏î‡πâ‡∏≠‡∏¢:**
- ‚ùå Sample efficiency ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ off-policy methods
- ‚ùå Requires on-policy data collection

**‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:**
- ‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏î cryptocurrency
- Portfolio management
- Beginners ‡πÉ‡∏ô RL

**Hyperparameters ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:**
```python
PPO_PARAMS = {
    'learning_rate': 1e-4,      # Learning rate
    'n_steps': 1024,            # Steps per rollout
    'batch_size': 128,          # Mini-batch size
    'n_epochs': 4,              # Training epochs per update
    'gamma': 0.99,              # Discount factor
    'gae_lambda': 0.95,         # GAE parameter
    'clip_range': 0.2,          # PPO clipping parameter
    'ent_coef': 0.01,           # Entropy coefficient
}
```

---

## üé≠ Off-Policy Algorithms

### DQN (Deep Q Networks)

**‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:**
- Value-based method ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ neural network ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì Q-function
- ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ discrete action spaces
- ‡∏°‡∏µ Rainbow extensions (Dueling, Double-Q, Distributional DQN)

**‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô:**
- ‚úÖ Sample efficient
- ‚úÖ Stable with replay buffer
- ‚úÖ Good for discrete actions

**‡∏à‡∏∏‡∏î‡∏î‡πâ‡∏≠‡∏¢:**
- ‚ùå ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ discrete actions
- ‚ùå ‡∏≠‡∏≤‡∏à overestimate Q-values

**‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:**
- Trading decisions (buy/sell/hold)
- Portfolio rebalancing
- Rule-based strategies

### SAC (Soft Actor-Critic) ‚≠ê **‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Continuous Actions**

**‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:**
- Actor-critic method ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ maximum entropy principle
- ‡πÄ‡∏û‡∏¥‡πà‡∏° exploration ‡∏ú‡πà‡∏≤‡∏ô entropy regularization
- ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö continuous action spaces

**‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô:**
- ‚úÖ ‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö continuous actions
- ‚úÖ Good exploration
- ‚úÖ Sample efficient
- ‚úÖ Robust performance

**‡∏à‡∏∏‡∏î‡∏î‡πâ‡∏≠‡∏¢:**
- ‚ùå Complex hyperparameter tuning
- ‚ùå Higher computational cost

**‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:**
- Position sizing
- Portfolio allocation weights
- Continuous trading strategies

**Hyperparameters ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:**
```python
SAC_PARAMS = {
    'learning_rate': 3e-4,      # Learning rate
    'buffer_size': 100000,      # Replay buffer size
    'batch_size': 256,          # Batch size
    'tau': 0.005,               # Target network update rate
    'gamma': 0.99,              # Discount factor
    'train_freq': 1,            # Training frequency
    'ent_coef': 'auto',         # Automatic entropy tuning
}
```

### DDPG (Deep Deterministic Policy Gradient)

**‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:**
- Deterministic policy gradient ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö continuous actions
- ‡πÉ‡∏ä‡πâ actor-critic architecture
- Predecessor ‡∏Ç‡∏≠‡∏á SAC

**‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô:**
- ‚úÖ Deterministic policies
- ‚úÖ Continuous actions
- ‚úÖ Relatively simple

**‡∏à‡∏∏‡∏î‡∏î‡πâ‡∏≠‡∏¢:**
- ‚ùå Sensitive to hyperparameters
- ‚ùå ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ exploration
- ‚ùå Less stable than SAC

**‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:**
- Simple continuous control
- When deterministic policies preferred

---

## ‚ö° High-Throughput Algorithms

### APPO (Asynchronous PPO)

**‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:**
- Asynchronous version ‡∏Ç‡∏≠‡∏á PPO
- ‡πÉ‡∏ä‡πâ V-trace ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö off-policy correction
- ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö distributed training

**‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô:**
- ‚úÖ Scalable to many workers
- ‚úÖ High throughput
- ‚úÖ Handles off-policy data

**‡∏à‡∏∏‡∏î‡∏î‡πâ‡∏≠‡∏¢:**
- ‚ùå Complex implementation
- ‚ùå Requires distributed setup

**‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:**
- Large-scale trading systems
- High-frequency trading
- When speed is critical

### IMPALA (Importance Weighted Actor-Learner Architecture)

**‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:**
- Highly scalable distributed RL
- ‡πÉ‡∏ä‡πâ V-trace ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö off-policy correction
- ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢ environments ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô

**‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô:**
- ‚úÖ Extremely scalable
- ‚úÖ High sample throughput
- ‚úÖ Stable training

**‡∏à‡∏∏‡∏î‡∏î‡πâ‡∏≠‡∏¢:**
- ‚ùå ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ discrete actions ‡∏´‡∏•‡∏±‡∏Å
- ‚ùå Complex infrastructure requirements

**‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:**
- Massive parallel trading
- Multiple market environments
- Research applications

---

## üîÆ Model-based RL

### DreamerV3

**‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:**
- ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ world model ‡∏Ç‡∏≠‡∏á environment
- ‡πÉ‡∏ä‡πâ model ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÑ‡∏î‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠ plan ‡πÅ‡∏•‡∏∞ train policy
- Sample efficient ‡∏°‡∏≤‡∏Å

**‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô:**
- ‚úÖ Very sample efficient
- ‚úÖ ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á continuous ‡πÅ‡∏•‡∏∞ discrete
- ‚úÖ Can learn from complex observations

**‡∏à‡∏∏‡∏î‡∏î‡πâ‡∏≠‡∏¢:**
- ‚ùå Complex implementation
- ‚ùå Model bias ‡∏õ‡∏±‡∏ç‡∏´‡∏≤
- ‚ùå Higher computational cost

**‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:**
- Complex market dynamics
- When sample efficiency critical
- Research applications

---

## üìö Offline RL ‡πÅ‡∏•‡∏∞ Imitation Learning

### BC (Behavior Cloning)

**‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:**
- ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£ imitate expert behavior
- Supervised learning approach
- ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ reward signal

**‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô:**
- ‚úÖ Simple implementation
- ‚úÖ Stable training
- ‚úÖ No environment interaction needed

**‡∏à‡∏∏‡∏î‡∏î‡πâ‡∏≠‡∏¢:**
- ‚ùå Limited to expert performance
- ‚ùå Distribution shift problems
- ‚ùå No improvement beyond data

**‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:**
- Imitating successful traders
- Learning from historical strategies
- Initial policy training

### MARWIL (Monotonic Advantage Re-Weighted Imitation Learning)

**‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:**
- ‡∏ú‡∏™‡∏° imitation learning ‡∏Å‡∏±‡∏ö policy gradient
- ‡πÉ‡∏ä‡πâ advantage weighting
- ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å offline data ‡∏û‡∏£‡πâ‡∏≠‡∏° rewards

**‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô:**
- ‚úÖ Better than pure BC
- ‚úÖ Uses reward information
- ‚úÖ Monotonic improvement

**‡∏à‡∏∏‡∏î‡∏î‡πâ‡∏≠‡∏¢:**
- ‚ùå Still limited by data quality
- ‚ùå More complex than BC

**‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:**
- Learning from profitable trades
- Improving existing strategies
- Risk-aware trading

### CQL (Conservative Q-Learning)

**‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:**
- Offline RL ‡∏ó‡∏µ‡πà‡∏•‡∏î overestimation ‡∏Ç‡∏≠‡∏á Q-values
- ‡πÄ‡∏û‡∏¥‡πà‡∏° conservative penalty
- ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö offline datasets

**‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô:**
- ‚úÖ Handles distribution shift
- ‚úÖ Conservative estimates
- ‚úÖ Works with suboptimal data

**‡∏à‡∏∏‡∏î‡∏î‡πâ‡∏≠‡∏¢:**
- ‚ùå ‡∏≠‡∏≤‡∏à too conservative
- ‚ùå Complex hyperparameter tuning

**‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:**
- Learning from historical market data
- Risk-averse strategies
- Backtesting improvements

---

## üîß Algorithm Extensions

### ICM (Intrinsic Curiosity Module)

**‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:**
- ‡πÄ‡∏û‡∏¥‡πà‡∏° curiosity-driven exploration
- ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ world model ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á intrinsic rewards
- ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö algorithm ‡∏≠‡∏∑‡πà‡∏ô‡πÑ‡∏î‡πâ

**‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô:**
- ‚úÖ Better exploration
- ‚úÖ Works with any base algorithm
- ‚úÖ Handles sparse rewards

**‡∏à‡∏∏‡∏î‡∏î‡πâ‡∏≠‡∏¢:**
- ‚ùå Additional computational cost
- ‚ùå May distract from main objective

**‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:**
- Markets with sparse signals
- Discovering new trading patterns
- Research applications

---

## üéØ ‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Algorithm

### ‡∏ï‡∏≤‡∏° Action Space:

| Action Type | ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ Algorithms |
|-------------|------------------|
| **Discrete** (Buy/Sell/Hold) | PPO, DQN, IMPALA |
| **Continuous** (Position sizes) | SAC, DDPG, PPO |
| **Mixed** | PPO, DreamerV3 |

### ‡∏ï‡∏≤‡∏° Data Availability:

| Data Type | ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ Algorithms |
|-----------|------------------|
| **Online Learning** | PPO, SAC, DQN |
| **Offline Only** | BC, MARWIL, CQL |
| **Mixed** | MARWIL, CQL |

### ‡∏ï‡∏≤‡∏° Performance Requirements:

| Priority | ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ Algorithms |
|----------|------------------|
| **Sample Efficiency** | SAC, DreamerV3, CQL |
| **Training Speed** | PPO, APPO |
| **Scalability** | IMPALA, APPO |
| **Stability** | PPO, BC |

### ‡∏ï‡∏≤‡∏° Experience Level:

| Level | ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ Algorithms |
|-------|------------------|
| **Beginner** | PPO, BC |
| **Intermediate** | SAC, DQN, MARWIL |
| **Advanced** | DreamerV3, APPO, IMPALA |

---

## üèóÔ∏è ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÉ‡∏ô finrl_minimal_crypto

### Current Implementation:

```python
# main.py - ‡πÉ‡∏ä‡πâ PPO ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å
model_name = "ppo"
model = agent.get_model(model_name, model_kwargs=PPO_PARAMS)
```

### Notebooks Configuration:

```python
# notebooks/config.py - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö 4 algorithms
def get_model_params(model_name):
    params_map = {
        'PPO': PPO_PARAMS,
        'A2C': A2C_PARAMS, 
        'DDPG': DDPG_PARAMS,
        'SAC': SAC_PARAMS
    }
    return params_map.get(model_name.upper(), PPO_PARAMS)
```

### Streamlit UI:

- ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö algorithm ‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢
- Grade system: N, D, C, B, A, S
- Interactive parameter tuning

---

## üí° ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô

### 1. ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô Algorithm ‡πÉ‡∏ô main.py:

```python
# ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà PPO ‡∏î‡πâ‡∏ß‡∏¢ SAC
model_name = "sac"
SAC_PARAMS = {
    'learning_rate': 3e-4,
    'buffer_size': 50000,
    'batch_size': 256,
    'tau': 0.005,
}
model = agent.get_model(model_name, model_kwargs=SAC_PARAMS)
```

### 2. ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ Multiple Algorithms:

```python
# ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö algorithms ‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏±‡∏ß
algorithms = ['PPO', 'SAC', 'A2C']
results = {}

for algo in algorithms:
    model_params = get_model_params(algo)
    model = agent.get_model(algo.lower(), model_kwargs=model_params)
    # Train and evaluate
    results[algo] = evaluate_model(model)
```

### 3. ‡∏Å‡∏≤‡∏£ Fine-tune Hyperparameters:

```python
# Grid search ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö PPO
from ray import tune

ppo_config = {
    'learning_rate': tune.grid_search([1e-5, 1e-4, 1e-3]),
    'n_steps': tune.grid_search([512, 1024, 2048]),
    'batch_size': tune.grid_search([64, 128, 256]),
}
```

---

## üìà Performance Comparison

| Algorithm | Sample Efficiency | Training Speed | Stability | Continuous Actions |
|-----------|------------------|----------------|-----------|-------------------|
| **PPO** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **SAC** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **DQN** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ùå |
| **A2C** | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| **DDPG** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |

---

## üîó ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°

### Documentation:
- [FinRL Official Documentation](https://finrl.readthedocs.io/)
- [Stable Baselines3 Documentation](https://stable-baselines3.readthedocs.io/)
- [Ray RLlib Documentation](https://docs.ray.io/en/latest/rllib/)

### Research Papers:
- PPO: [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)
- SAC: [Soft Actor-Critic Algorithms](https://arxiv.org/abs/1812.05905)
- DQN: [Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602)

### ‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°:
```bash
# ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö advanced algorithms
pip install ray[rllib]
pip install tensorboard

# ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö hyperparameter tuning
pip install optuna
pip install wandb
```

---

## üìù ‡∏™‡∏£‡∏∏‡∏õ

‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å algorithm ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö:

1. **‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡∏≠‡∏á action space** (discrete vs continuous)
2. **‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ** (online vs offline)
3. **‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô** (sample efficiency vs speed)
4. **‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á environment**
5. **‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ**

‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö **cryptocurrency trading** ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ **PPO** ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏™‡∏π‡∏á‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢ ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡∏Ñ‡πà‡∏≠‡∏¢‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏Å‡∏±‡∏ö **SAC** ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö continuous actions ‡∏´‡∏£‡∏∑‡∏≠ **DQN** ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö discrete actions ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£

---

*‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ô‡∏µ‡πâ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î: ‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏° 2025*
*‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ: finrl_minimal_crypto* 