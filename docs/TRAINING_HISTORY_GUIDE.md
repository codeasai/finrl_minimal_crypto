# ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏î‡∏π‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£ Train ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û Model

> ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Training History ‡πÅ‡∏•‡∏∞ Performance ‡∏Ç‡∏≠‡∏á Models ‡πÉ‡∏ô‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ finrl_minimal_crypto

## üìã ‡∏™‡∏≤‡∏£‡∏ö‡∏±‡∏ç

1. [‡∏£‡∏∞‡∏ö‡∏ö Tracking ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà](#‡∏£‡∏∞‡∏ö‡∏ö-tracking-‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà)
2. [‡∏Å‡∏≤‡∏£‡∏î‡∏π Training Logs](#‡∏Å‡∏≤‡∏£‡∏î‡∏π-training-logs)
3. [‡∏Å‡∏≤‡∏£‡∏î‡∏π Model Performance](#‡∏Å‡∏≤‡∏£‡∏î‡∏π-model-performance)
4. [‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ Tensorboard](#‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ-tensorboard)
5. [‡∏Å‡∏≤‡∏£‡∏î‡∏π Saved Models](#‡∏Å‡∏≤‡∏£‡∏î‡∏π-saved-models)
6. [‡∏Å‡∏≤‡∏£ Compare Models](#‡∏Å‡∏≤‡∏£-compare-models)
7. [‡∏Å‡∏≤‡∏£ Export ‡πÅ‡∏•‡∏∞ Analysis](#‡∏Å‡∏≤‡∏£-export-‡πÅ‡∏•‡∏∞-analysis)
8. [Advanced Monitoring](#advanced-monitoring)

---

## üéØ ‡∏£‡∏∞‡∏ö‡∏ö Tracking ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà

### ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°

```
finrl_minimal_crypto/
‚îú‚îÄ‚îÄ logs/                           # Training logs
‚îÇ   ‚îú‚îÄ‚îÄ sac_enhanced_env/          # Enhanced environment logs
‚îÇ   ‚îú‚îÄ‚îÄ sac_original_env/          # Original environment logs  
‚îÇ   ‚îú‚îÄ‚îÄ sac_eval_grade_A/          # Grade A evaluation logs
‚îÇ   ‚îî‚îÄ‚îÄ sac_graded/               # Graded training logs
‚îú‚îÄ‚îÄ models/                        # Saved models
‚îÇ   ‚îú‚îÄ‚îÄ sac/                      # SAC models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metadata/             # Model metadata
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ best_enhanced_env/    # Best enhanced models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ best_original_env/    # Best original models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ *.zip                 # Model files
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ *_info.pkl           # Model information
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ *.png                # Performance charts
‚îÇ   ‚îî‚îÄ‚îÄ *.zip                     # Other models (PPO, etc.)
‚îî‚îÄ‚îÄ ui/                           # Streamlit UI
    ‚îî‚îÄ‚îÄ pages/6_Manage_Agents.py  # Model management
```

### ‡∏£‡∏∞‡∏ö‡∏ö Metadata Management

‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ‡πÉ‡∏ä‡πâ **SAC Metadata Manager** ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°:
- ‚úÖ Configuration ‡πÅ‡∏•‡∏∞ hyperparameters
- ‚úÖ Training history ‡πÅ‡∏•‡∏∞ performance metrics
- ‚úÖ Evaluation results ‡πÅ‡∏•‡∏∞ backtest data
- ‚úÖ Grade-based organization (N, D, C, B, A, S)
- ‚úÖ Model versioning ‡πÅ‡∏•‡∏∞ timestamps

---

## üìä ‡∏Å‡∏≤‡∏£‡∏î‡∏π Training Logs

### 1. ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡πá‡∏ß - ‡πÉ‡∏ä‡πâ test_sac_results.py

```bash
python test_sac_results.py
```

**‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ:**
```
üîç ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå SAC Agent
==================================================
üìÅ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å: sac_agent_20250619_151128_XXBF8G_info.pkl

üìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• SAC Agent:
------------------------------
üî§ ‡∏ä‡∏∑‡πà‡∏≠ Model: sac_agent_20250619_151128_XXBF8G
ü§ñ Algorithm: SAC
üìÖ ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á: 2025-06-19 15:11:28
üí∞ ‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô: $100,000
üí∏ ‡∏Ñ‡πà‡∏≤‡∏ò‡∏£‡∏£‡∏°‡πÄ‡∏ô‡∏µ‡∏¢‡∏°: 0.1%
üìà ‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå: ['BTC-USD']
```

### 2. ‡∏î‡∏π‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå logs ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î

```bash
# ‡∏î‡∏π logs directories
ls -la logs/

# ‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î logs
ls -la logs/sac_enhanced_env/
ls -la logs/sac_original_env/
```

### 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå‡∏î‡∏π logs ‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î

```python
# view_training_logs.py
import os
import json
from datetime import datetime

def view_training_logs():
    """‡∏î‡∏π training logs ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"""
    
    print("üìä Training Logs Overview")
    print("=" * 50)
    
    logs_dir = "logs"
    
    for subdir in os.listdir(logs_dir):
        subdir_path = os.path.join(logs_dir, subdir)
        if os.path.isdir(subdir_path):
            print(f"\nüìÅ {subdir}/")
            
            # ‡∏ô‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡∏ô‡∏≤‡∏î
            total_files = 0
            total_size = 0
            
            for root, dirs, files in os.walk(subdir_path):
                total_files += len(files)
                for file in files:
                    total_size += os.path.getsize(os.path.join(root, file))
            
            print(f"  üìÑ Files: {total_files}")
            print(f"  üíæ Total Size: {total_size / (1024*1024):.1f} MB")
            
            # ‡πÅ‡∏™‡∏î‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
            latest_files = []
            for root, dirs, files in os.walk(subdir_path):
                for file in files:
                    file_path = os.path.join(root, file)
                    mod_time = os.path.getmtime(file_path)
                    latest_files.append((file, mod_time))
            
            if latest_files:
                latest_files.sort(key=lambda x: x[1], reverse=True)
                latest_file, latest_time = latest_files[0]
                latest_datetime = datetime.fromtimestamp(latest_time)
                print(f"  üïê Latest: {latest_file} ({latest_datetime.strftime('%Y-%m-%d %H:%M')})")

if __name__ == "__main__":
    view_training_logs()
```

---

## ü§ñ ‡∏Å‡∏≤‡∏£‡∏î‡∏π Model Performance

### 1. ‡πÉ‡∏ä‡πâ SAC Metadata Manager

```python
# view_model_performance.py
from models.sac.sac_metadata_manager import SAC_MetadataManager

def view_all_models():
    """‡∏î‡∏π‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û models ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"""
    
    manager = SAC_MetadataManager()
    manager.load_all_metadata()
    
    agents = manager.list_agents()
    
    print("ü§ñ SAC Models Overview")
    print("=" * 50)
    print(f"Total Models: {len(agents)}")
    
    if not agents:
        print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö SAC models")
        return
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏ï‡∏≤‡∏° grade
    grade_stats = manager.get_grade_statistics()
    
    print(f"\nüìä Statistics by Grade:")
    print("-" * 30)
    
    for grade, stats in grade_stats.items():
        print(f"Grade {grade}: {stats['count']} models")
        print(f"  üìà Mean Reward: {stats['mean_reward']:.4f}")
        print(f"  üèÜ Best Reward: {stats['best_reward']:.4f}")
        print(f"  ‚úÖ Success Rate: {stats['success_rate']:.2%}")
        print(f"  ‚è±Ô∏è  Avg Duration: {stats['avg_training_duration']:.0f}s")
        print()
    
    # ‡πÅ‡∏™‡∏î‡∏á models ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
    print(f"üèÖ Best Models:")
    print("-" * 30)
    
    best_agents = manager.get_best_agents_by_grade(top_n=2)
    
    for grade, agents_list in best_agents.items():
        if agents_list:
            print(f"Grade {grade}:")
            for i, agent in enumerate(agents_list, 1):
                reward = agent.performance_metrics.get('mean_reward', 0)
                duration = agent.get_training_duration_formatted()
                print(f"  {i}. {agent.agent_id[:20]}...")
                print(f"     Reward: {reward:.4f}, Duration: {duration}")
            print()

if __name__ == "__main__":
    view_all_models()
```

### 2. ‡∏î‡∏π Performance Charts

```python
# view_charts.py
import os
import subprocess
from datetime import datetime

def view_performance_charts():
    """‡πÅ‡∏™‡∏î‡∏á performance charts"""
    
    charts_dir = "models/sac"
    chart_files = [f for f in os.listdir(charts_dir) if f.endswith('.png')]
    
    print("üìä Available Performance Charts")
    print("=" * 40)
    
    if not chart_files:
        print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö performance charts")
        return
    
    for i, chart_file in enumerate(chart_files, 1):
        file_path = os.path.join(charts_dir, chart_file)
        file_size = os.path.getsize(file_path) / 1024  # KB
        mod_time = datetime.fromtimestamp(os.path.getmtime(file_path))
        
        print(f"{i:2d}. {chart_file}")
        print(f"     Size: {file_size:.1f} KB")
        print(f"     Modified: {mod_time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    # ‡πÄ‡∏õ‡∏¥‡∏î‡∏î‡∏π chart ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
    if chart_files:
        latest_chart = max(chart_files, 
                          key=lambda x: os.path.getmtime(os.path.join(charts_dir, x)))
        
        print(f"\nüñºÔ∏è  Opening latest chart: {latest_chart}")
        chart_path = os.path.join(charts_dir, latest_chart)
        
        # ‡πÄ‡∏õ‡∏¥‡∏î‡∏î‡πâ‡∏ß‡∏¢ default image viewer
        try:
            subprocess.run(['start', chart_path], shell=True)
        except:
            print(f"üìÅ Chart location: {chart_path}")

if __name__ == "__main__":
    view_performance_charts()
```

---

## üìà ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ Tensorboard

### 1. ‡πÄ‡∏£‡∏¥‡πà‡∏° Tensorboard

```bash
# ‡∏£‡∏±‡∏ô tensorboard (‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô)
tensorboard --logdir=logs

# ‡∏£‡∏±‡∏ô‡πÅ‡∏ö‡∏ö‡∏£‡∏∞‡∏ö‡∏∏ port
tensorboard --logdir=logs --port=6006

# ‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ô browser
start http://localhost:6006
```

### 2. ‡∏î‡∏π Metrics ‡πÉ‡∏ô Tensorboard

**Metrics ‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏π‡πÑ‡∏î‡πâ:**
- üìä **rollout/ep_rew_mean**: Average episode reward
- üìà **train/learning_rate**: Learning rate over time
- üß† **train/policy_loss**: Policy network loss
- üîç **train/value_loss**: Value network loss
- üìâ **train/entropy_loss**: Entropy loss

### 3. Compare ‡∏´‡∏•‡∏≤‡∏¢ Experiments

```python
# organize_tensorboard.py
import os

def organize_tensorboard_runs():
    """‡∏à‡∏±‡∏î‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö tensorboard logs ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö comparison"""
    
    base_dir = "logs"
    comparison_dir = "logs/comparison"
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á comparison directory
    os.makedirs(comparison_dir, exist_ok=True)
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á symbolic links
    experiments = {
        "Enhanced_SAC": "sac_enhanced_env",
        "Original_SAC": "sac_original_env",
        "Grade_A_SAC": "sac_eval_grade_A"
    }
    
    for exp_name, source_dir in experiments.items():
        source_path = os.path.join(base_dir, source_dir)
        target_path = os.path.join(comparison_dir, exp_name)
        
        if os.path.exists(source_path) and not os.path.exists(target_path):
            try:
                os.symlink(os.path.abspath(source_path), target_path)
                print(f"‚úÖ Linked {exp_name}")
            except:
                print(f"‚ùå Failed to link {exp_name}")
    
    print(f"\nüöÄ Run comparison with:")
    print(f"tensorboard --logdir={comparison_dir}")

if __name__ == "__main__":
    organize_tensorboard_runs()
```

---

## üíæ ‡∏Å‡∏≤‡∏£‡∏î‡∏π Saved Models

### 1. ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ Models

```python
# list_models.py
import os
import pickle
from datetime import datetime

def list_all_models():
    """‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ models ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"""
    
    print("ü§ñ All Saved Models")
    print("=" * 60)
    
    # SAC Models
    sac_dir = "models/sac"
    if os.path.exists(sac_dir):
        print("\nüß† SAC Models:")
        print("-" * 30)
        
        sac_files = [f for f in os.listdir(sac_dir) if f.endswith('.zip')]
        sac_files.sort(key=lambda x: os.path.getmtime(os.path.join(sac_dir, x)), reverse=True)
        
        for i, model_file in enumerate(sac_files, 1):
            model_path = os.path.join(sac_dir, model_file)
            file_size = os.path.getsize(model_path) / (1024*1024)  # MB
            mod_time = datetime.fromtimestamp(os.path.getmtime(model_path))
            
            print(f"{i:2d}. {model_file}")
            print(f"     Size: {file_size:.1f} MB")
            print(f"     Modified: {mod_time.strftime('%Y-%m-%d %H:%M:%S')}")
            
            # ‡∏î‡∏π metadata ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
            info_file = model_file.replace('.zip', '_info.pkl')
            info_path = os.path.join(sac_dir, info_file)
            
            if os.path.exists(info_path):
                try:
                    with open(info_path, 'rb') as f:
                        info = pickle.load(f)
                    print(f"     Created: {info.get('created_date', 'N/A')[:19]}")
                    print(f"     Symbols: {info.get('crypto_symbols', 'N/A')}")
                except:
                    pass
            print()
    
    # PPO Models
    models_dir = "models"
    ppo_files = [f for f in os.listdir(models_dir) 
                 if f.endswith('.zip') and ('ppo' in f.lower() or 'PPO' in f)]
    
    if ppo_files:
        print("üèÉ PPO Models:")
        print("-" * 30)
        
        for i, model_file in enumerate(ppo_files, 1):
            model_path = os.path.join(models_dir, model_file)
            file_size = os.path.getsize(model_path) / (1024*1024)  # MB
            mod_time = datetime.fromtimestamp(os.path.getmtime(model_path))
            
            print(f"{i:2d}. {model_file}")
            print(f"     Size: {file_size:.1f} MB")
            print(f"     Modified: {mod_time.strftime('%Y-%m-%d %H:%M:%S')}")
            print()

if __name__ == "__main__":
    list_all_models()
```

### 2. ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏ó‡∏î‡∏™‡∏≠‡∏ö Model

```python
# test_model.py
from stable_baselines3 import SAC
import os
import pickle

def load_and_test_model(model_name=None):
    """‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏ó‡∏î‡∏™‡∏≠‡∏ö model"""
    
    sac_dir = "models/sac"
    
    if model_name is None:
        # ‡∏´‡∏≤ model ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
        sac_files = [f for f in os.listdir(sac_dir) if f.endswith('.zip')]
        if not sac_files:
            print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö SAC models")
            return None
        
        latest_file = max(sac_files, 
                         key=lambda x: os.path.getmtime(os.path.join(sac_dir, x)))
        model_name = latest_file.replace('.zip', '')
    
    model_path = os.path.join(sac_dir, f"{model_name}.zip")
    info_path = os.path.join(sac_dir, f"{model_name}_info.pkl")
    
    print(f"ü§ñ Loading Model: {model_name}")
    print("=" * 50)
    
    # ‡∏î‡∏π model info
    if os.path.exists(info_path):
        try:
            with open(info_path, 'rb') as f:
                info = pickle.load(f)
            
            print("üìä Model Information:")
            print(f"  Algorithm: {info.get('algorithm', 'N/A')}")
            print(f"  Created: {info.get('created_date', 'N/A')}")
            print(f"  Initial Amount: ${info.get('initial_amount', 0):,}")
            print(f"  Crypto Symbols: {info.get('crypto_symbols', 'N/A')}")
            print(f"  Train Data: {info.get('train_data_shape', 'N/A')}")
            print(f"  Test Data: {info.get('test_data_shape', 'N/A')}")
            
        except Exception as e:
            print(f"‚ö†Ô∏è  Cannot read model info: {e}")
    
    # ‡πÇ‡∏´‡∏•‡∏î model
    if os.path.exists(model_path):
        try:
            model = SAC.load(model_path)
            print(f"\n‚úÖ Model loaded successfully!")
            
            print(f"\nModel Details:")
            print(f"  Policy: {model.policy.__class__.__name__}")
            print(f"  Learning Rate: {model.learning_rate}")
            print(f"  Buffer Size: {model.buffer_size:,}")
            print(f"  Batch Size: {model.batch_size}")
            
            return model
            
        except Exception as e:
            print(f"‚ùå Error loading model: {e}")
            return None
    else:
        print(f"‚ùå Model file not found: {model_path}")
        return None

if __name__ == "__main__":
    # ‡πÉ‡∏ä‡πâ model ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
    model = load_and_test_model()
    
    # ‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏∞‡∏ö‡∏∏ model name
    # model = load_and_test_model("sac_agent_20250619_151128_XXBF8G")
```

---

## üÜö ‡∏Å‡∏≤‡∏£ Compare Models

### 1. ‡πÉ‡∏ä‡πâ SAC Metadata Manager

```python
# compare_models.py
from models.sac.sac_metadata_manager import SAC_MetadataManager
import pandas as pd
import matplotlib.pyplot as plt

def compare_models():
    """‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö models"""
    
    manager = SAC_MetadataManager()
    manager.load_all_metadata()
    
    agents = manager.list_agents()
    
    if len(agents) < 2:
        print("‚ùå ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 2 models ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö")
        return
    
    print("üÜö Model Comparison")
    print("=" * 60)
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á comparison table
    comparison_df = manager.get_performance_comparison()
    
    if comparison_df.empty:
        print("‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö")
        return
    
    print("\nüìä Performance Comparison:")
    print(comparison_df.to_string())
    
    # ‡∏´‡∏≤ top performers
    if 'mean_reward' in comparison_df.columns:
        top_performers = comparison_df.nlargest(3, 'mean_reward')
        
        print(f"\nüèÜ Top 3 Performers:")
        print("-" * 30)
        for i, (agent_id, row) in enumerate(top_performers.iterrows(), 1):
            print(f"{i}. {agent_id}")
            print(f"   Mean Reward: {row['mean_reward']:.4f}")
            print(f"   Best Reward: {row.get('best_reward', 'N/A')}")
            print(f"   Grade: {row.get('grade', 'N/A')}")
            print()
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö
    if len(comparison_df) > 1:
        create_comparison_chart(comparison_df)
    
    return comparison_df

def create_comparison_chart(df):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö"""
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('Model Performance Comparison', fontsize=16)
    
    # Mean Reward
    if 'mean_reward' in df.columns:
        axes[0,0].bar(range(len(df)), df['mean_reward'])
        axes[0,0].set_title('Mean Reward')
        axes[0,0].set_xticks(range(len(df)))
        axes[0,0].set_xticklabels([idx[:10] + '...' for idx in df.index], rotation=45)
    
    # Best Reward
    if 'best_reward' in df.columns:
        axes[0,1].bar(range(len(df)), df['best_reward'])
        axes[0,1].set_title('Best Reward')
        axes[0,1].set_xticks(range(len(df)))
        axes[0,1].set_xticklabels([idx[:10] + '...' for idx in df.index], rotation=45)
    
    # Stability Score
    if 'stability_score' in df.columns:
        axes[1,0].bar(range(len(df)), df['stability_score'])
        axes[1,0].set_title('Stability Score')
        axes[1,0].set_xticks(range(len(df)))
        axes[1,0].set_xticklabels([idx[:10] + '...' for idx in df.index], rotation=45)
    
    # Training Duration
    if 'training_duration' in df.columns:
        axes[1,1].bar(range(len(df)), df['training_duration'])
        axes[1,1].set_title('Training Duration (seconds)')
        axes[1,1].set_xticks(range(len(df)))
        axes[1,1].set_xticklabels([idx[:10] + '...' for idx in df.index], rotation=45)
    
    plt.tight_layout()
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Å‡∏£‡∏≤‡∏ü
    output_path = "models/sac/comparison_chart.png"
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"\nüìä Comparison chart saved: {output_path}")
    
    plt.show()

if __name__ == "__main__":
    comparison_df = compare_models()
```

### 2. ‡πÉ‡∏ä‡πâ Streamlit UI

```bash
# ‡πÄ‡∏õ‡∏¥‡∏î Streamlit UI
cd ui
streamlit run app.py
```

‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏´‡∏ô‡πâ‡∏≤ **"6_Manage_Agents"** ‡πÄ‡∏û‡∏∑‡πà‡∏≠:
- ‚úÖ ‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ models ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
- ‚úÖ ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û  
- ‚úÖ ‡∏î‡∏π training history
- ‚úÖ ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏•‡πå models

---

## üì§ ‡∏Å‡∏≤‡∏£ Export ‡πÅ‡∏•‡∏∞ Analysis

### 1. Export ‡πÄ‡∏õ‡πá‡∏ô CSV

```python
# export_data.py
from models.sac.sac_metadata_manager import SAC_MetadataManager
from datetime import datetime

def export_model_data():
    """Export ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• models ‡πÄ‡∏õ‡πá‡∏ô CSV"""
    
    manager = SAC_MetadataManager()
    manager.load_all_metadata()
    
    # Export performance data
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_file = f"model_performance_{timestamp}.csv"
    
    manager.export_to_csv(output_file)
    
    print(f"‚úÖ Model data exported to: {output_file}")
    
    # ‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
    import pandas as pd
    df = pd.read_csv(output_file)
    
    print(f"\nüìä Export Summary:")
    print(f"  Total Models: {len(df)}")
    print(f"  Columns: {list(df.columns)}")
    print(f"\nFirst 3 rows:")
    print(df.head(3))
    
    return output_file

if __name__ == "__main__":
    export_file = export_model_data()
```

### 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á Performance Report

```python
# generate_report.py
import os
import matplotlib.pyplot as plt
from datetime import datetime
from models.sac.sac_metadata_manager import SAC_MetadataManager

def generate_performance_report():
    """‡∏™‡∏£‡πâ‡∏≤‡∏á performance report"""
    
    manager = SAC_MetadataManager()
    manager.load_all_metadata()
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    report_dir = f"performance_report_{timestamp}"
    os.makedirs(report_dir, exist_ok=True)
    
    print(f"üìä Generating Performance Report...")
    print(f"üìÅ Output: {report_dir}")
    
    # 1. ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    agents = manager.list_agents()
    grade_stats = manager.get_grade_statistics()
    comparison_df = manager.get_performance_comparison()
    
    # 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('SAC Models Performance Report', fontsize=16)
    
    # Grade distribution
    grades = list(grade_stats.keys())
    counts = [grade_stats[g]['count'] for g in grades]
    
    if grades:
        axes[0,0].pie(counts, labels=grades, autopct='%1.1f%%')
        axes[0,0].set_title('Models by Grade')
        
        # Mean reward by grade
        mean_rewards = [grade_stats[g]['mean_reward'] for g in grades]
        axes[0,1].bar(grades, mean_rewards)
        axes[0,1].set_title('Mean Reward by Grade')
        
        # Success rate by grade
        success_rates = [grade_stats[g]['success_rate'] * 100 for g in grades]
        axes[1,0].bar(grades, success_rates)
        axes[1,0].set_title('Success Rate by Grade (%)')
        
        # Training duration
        durations = [grade_stats[g]['avg_training_duration'] / 60 for g in grades]  # minutes
        axes[1,1].bar(grades, durations)
        axes[1,1].set_title('Avg Training Duration (minutes)')
    
    plt.tight_layout()
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Å‡∏£‡∏≤‡∏ü
    chart_path = os.path.join(report_dir, 'performance_analysis.png')
    plt.savefig(chart_path, dpi=300, bbox_inches='tight')
    print(f"üìä Chart saved: {chart_path}")
    
    # 3. Export data
    csv_path = os.path.join(report_dir, 'model_data.csv')
    manager.export_to_csv(csv_path)
    print(f"üìã Data exported: {csv_path}")
    
    # 4. ‡∏™‡∏£‡πâ‡∏≤‡∏á text report
    report_path = os.path.join(report_dir, 'report.txt')
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("SAC Models Performance Report\n")
        f.write("=" * 50 + "\n")
        f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        f.write(f"Total Models: {len(agents)}\n")
        f.write(f"Grades Available: {', '.join(grades)}\n\n")
        
        f.write("Grade Statistics:\n")
        f.write("-" * 30 + "\n")
        for grade, stats in grade_stats.items():
            f.write(f"Grade {grade}:\n")
            f.write(f"  Count: {stats['count']}\n")
            f.write(f"  Mean Reward: {stats['mean_reward']:.4f}\n")
            f.write(f"  Best Reward: {stats['best_reward']:.4f}\n")
            f.write(f"  Success Rate: {stats['success_rate']:.2%}\n\n")
        
        # Best models
        best_agents = manager.get_best_agents_by_grade(top_n=2)
        f.write("Best Models by Grade:\n")
        f.write("-" * 30 + "\n")
        for grade, agents_list in best_agents.items():
            f.write(f"Grade {grade}:\n")
            for i, agent in enumerate(agents_list, 1):
                f.write(f"  {i}. {agent.agent_id}\n")
                f.write(f"     Mean Reward: {agent.performance_metrics.get('mean_reward', 0):.4f}\n")
                f.write(f"     Duration: {agent.get_training_duration_formatted()}\n")
            f.write("\n")
    
    print(f"üìù Report saved: {report_path}")
    print(f"\n‚úÖ Performance report completed!")
    
    return report_dir

if __name__ == "__main__":
    report_dir = generate_performance_report()
```

---

## üéØ ‡∏™‡∏£‡∏∏‡∏õ - ‡∏ß‡∏¥‡∏ò‡∏µ‡∏î‡∏π‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡πá‡∏ß

### üöÄ ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô

```bash
# 1. ‡∏î‡∏π model ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î (‡πÄ‡∏£‡πá‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)
python test_sac_results.py

# 2. ‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ models ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
python -c "
import os
sac_files = [f for f in os.listdir('models/sac') if f.endswith('.zip')]
print(f'Found {len(sac_files)} SAC models')
for f in sorted(sac_files, reverse=True)[:5]:
    print(f'  {f}')
"

# 3. ‡πÄ‡∏£‡∏¥‡πà‡∏° tensorboard
tensorboard --logdir=logs --port=6006

# 4. ‡πÄ‡∏õ‡∏¥‡∏î Streamlit UI
cd ui && streamlit run app.py
```

### üìä Python Scripts ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå

```python
# quick_model_check.py - ‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏ß‡∏î‡πÄ‡∏£‡πá‡∏ß
from models.sac.sac_metadata_manager import SAC_MetadataManager

manager = SAC_MetadataManager()
manager.load_all_metadata()

agents = manager.list_agents()
print(f"Total models: {len(agents)}")

if agents:
    latest = max(agents, key=lambda x: x.updated_at)
    print(f"Latest: {latest.agent_id}")
    print(f"Grade: {latest.grade}")
    print(f"Mean reward: {latest.performance_metrics.get('mean_reward', 0):.4f}")
```

### üé® ‡∏Å‡∏≤‡∏£‡∏î‡∏π Visualizations

1. **Performance Charts**: `models/sac/*.png`
2. **Tensorboard**: `http://localhost:6006` ‡∏´‡∏•‡∏±‡∏á‡∏£‡∏±‡∏ô `tensorboard --logdir=logs`
3. **Streamlit UI**: `http://localhost:8501` ‡∏´‡∏•‡∏±‡∏á‡∏£‡∏±‡∏ô `streamlit run app.py`

### üìÅ ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç

```
logs/                    # Training logs (tensorboard)
models/sac/             # SAC models ‡πÅ‡∏•‡∏∞ metadata
  ‚îú‚îÄ‚îÄ *.zip             # Model files
  ‚îú‚îÄ‚îÄ *_info.pkl        # Model information
  ‚îú‚îÄ‚îÄ *.png             # Performance charts
  ‚îî‚îÄ‚îÄ metadata/         # Detailed metadata
ui/pages/6_Manage_Agents.py  # Streamlit model management
```

---

*‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏ô‡∏µ‡πâ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ó‡∏∏‡∏Å‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏î‡∏π‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û model ‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡πá‡∏ß‡πÑ‡∏õ‡∏à‡∏ô‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å! ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å `python test_sac_results.py` ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏≠‡∏¢‡πÉ‡∏ä‡πâ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£* üéâ 