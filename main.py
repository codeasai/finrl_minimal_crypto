# main.py
import sys
import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import os

# FinRL imports
from finrl.meta.data_processors.processor_yahoofinance import YahooFinanceProcessor
from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv
from finrl.agents.stablebaselines3.models import DRLAgent

# Import config
from config import *

# à¸ªà¸£à¹‰à¸²à¸‡à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¸ªà¸³à¸«à¸£à¸±à¸šà¹€à¸à¹‡à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥
DATA_DIR = "data"
if not os.path.exists(DATA_DIR):
    os.makedirs(DATA_DIR)

def download_crypto_data(force_download=False):
    """
    à¸”à¸²à¸§à¸™à¹Œà¹‚à¸«à¸¥à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ crypto à¹‚à¸”à¸¢à¹ƒà¸Šà¹‰ FinRL YahooFinanceProcessor
    à¸¡à¸µà¸•à¸±à¸§à¹€à¸¥à¸·à¸­à¸à¹ƒà¸«à¹‰à¸šà¸±à¸‡à¸„à¸±à¸šà¸”à¸²à¸§à¸™à¹Œà¹‚à¸«à¸¥à¸”à¹ƒà¸«à¸¡à¹ˆà¹„à¸”à¹‰à¸œà¹ˆà¸²à¸™ force_download
    """
    data_file = os.path.join(DATA_DIR, "crypto_data.csv")
    
    # à¸–à¹‰à¸²à¸¡à¸µà¹„à¸Ÿà¸¥à¹Œà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸­à¸¢à¸¹à¹ˆà¹à¸¥à¹‰à¸§à¹à¸¥à¸°à¹„à¸¡à¹ˆà¹„à¸”à¹‰à¸šà¸±à¸‡à¸„à¸±à¸šà¸”à¸²à¸§à¸™à¹Œà¹‚à¸«à¸¥à¸”à¹ƒà¸«à¸¡à¹ˆ
    if os.path.exists(data_file) and not force_download:
        print("ğŸ“‚ Loading existing data...")
        df = pd.read_csv(data_file)
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        print(f"âœ… Loaded {len(df)} rows of data")
        return df
    
    print("ğŸ“Š Downloading crypto data...")
    
    # à¸ªà¸£à¹‰à¸²à¸‡ data processor
    processor = YahooFinanceProcessor()
    
    # à¸”à¸²à¸§à¸™à¹Œà¹‚à¸«à¸¥à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥
    df = processor.download_data(
        ticker_list=CRYPTO_SYMBOLS,
        start_date=START_DATE,
        end_date=END_DATE,
        time_interval='1D'
    )
    
    # à¸šà¸±à¸™à¸—à¸¶à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥
    df.to_csv(data_file, index=False)
    print(f"ğŸ’¾ Saved data to {data_file}")
    
    print(f"âœ… Downloaded {len(df)} rows of data")
    print(f"Date range: {df['timestamp'].min()} to {df['timestamp'].max()}")
    print(f"Symbols: {df['tic'].unique()}")
    
    return df

def add_technical_indicators(df):
    """
    à¹€à¸à¸´à¹ˆà¸¡ technical indicators à¹à¸¥à¸° normalize à¸‚à¹‰à¸­à¸¡à¸¹à¸¥
    """
    print("ğŸ“ˆ Adding technical indicators...")
    
    processor = YahooFinanceProcessor()
    
    # à¹€à¸à¸´à¹ˆà¸¡ technical indicators
    df = processor.add_technical_indicator(df, INDICATORS)
    
    # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¹à¸¥à¸°à¹à¸›à¸¥à¸‡ timestamp à¹ƒà¸«à¹‰à¸–à¸¹à¸à¸•à¹‰à¸­à¸‡
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['date'] = df['timestamp'].dt.date
    
    # Normalize à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸£à¸²à¸„à¸²à¹à¸¥à¸° volume
    price_cols = ['open', 'high', 'low', 'close']
    df[price_cols] = df[price_cols].apply(lambda x: (x - x.mean()) / x.std())
    df['volume'] = (df['volume'] - df['volume'].mean()) / df['volume'].std()
    
    # Normalize technical indicators
    for indicator in INDICATORS:
        if indicator in df.columns:
            df[indicator] = (df[indicator] - df[indicator].mean()) / df[indicator].std()
    
    # à¹à¸—à¸™à¸—à¸µà¹ˆà¸„à¹ˆà¸² inf à¹à¸¥à¸° nan à¸”à¹‰à¸§à¸¢ 0
    df = df.replace([np.inf, -np.inf], np.nan)
    df = df.fillna(0)
    
    print(f"âœ… Added indicators: {INDICATORS}")
    print(f"âœ… Normalized price, volume and indicators")
    print(f"Final columns: {len(df.columns)} columns")
    
    return df

def create_environment(df):
    """
    à¸ªà¸£à¹‰à¸²à¸‡ trading environment
    """
    print("ğŸ›ï¸ Creating trading environment...")
    
    # à¹à¸šà¹ˆà¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸›à¹‡à¸™ train/test à¹‚à¸”à¸¢à¹ƒà¸Šà¹‰à¸ªà¸±à¸”à¸ªà¹ˆà¸§à¸™ 80/20
    train_size = int(len(df) * 0.8)
    train_df = df.iloc[:train_size].reset_index(drop=True)
    test_df = df.iloc[train_size:].reset_index(drop=True)
    
    # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¹ƒà¸«à¹‰à¹à¸™à¹ˆà¹ƒà¸ˆà¸§à¹ˆà¸² timestamp à¹à¸¥à¸° date à¹€à¸›à¹‡à¸™à¸£à¸¹à¸›à¹à¸šà¸šà¸—à¸µà¹ˆà¸–à¸¹à¸à¸•à¹‰à¸­à¸‡
    for data in [train_df, test_df]:
        data['timestamp'] = pd.to_datetime(data['timestamp'])
        data['date'] = data['timestamp'].dt.date
        # à¹€à¸£à¸µà¸¢à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸•à¸²à¸¡à¸§à¸±à¸™à¸—à¸µà¹ˆ
        data.sort_values(['date', 'tic'], inplace=True)
        data.reset_index(drop=True, inplace=True)
    
    print(f"ğŸ“š Training data: {len(train_df)} rows ({train_df['timestamp'].min()} to {train_df['timestamp'].max()})")
    print(f"ğŸ“ Testing data: {len(test_df)} rows ({test_df['timestamp'].min()} to {test_df['timestamp'].max()})")
    
    # à¸ªà¸£à¹‰à¸²à¸‡ environment à¸ªà¸³à¸«à¸£à¸±à¸š training
    env_kwargs = {
        "hmax": HMAX,
        "initial_amount": INITIAL_AMOUNT,
        "num_stock_shares": [0] * len(CRYPTO_SYMBOLS),
        "buy_cost_pct": [TRANSACTION_COST_PCT] * len(CRYPTO_SYMBOLS),
        "sell_cost_pct": [TRANSACTION_COST_PCT] * len(CRYPTO_SYMBOLS),
        "state_space": 1 + 2 * len(CRYPTO_SYMBOLS) + len(CRYPTO_SYMBOLS) * len(INDICATORS),
        "stock_dim": len(CRYPTO_SYMBOLS),
        "tech_indicator_list": INDICATORS,
        "action_space": len(CRYPTO_SYMBOLS),
        "reward_scaling": 1e-3,  # à¸›à¸£à¸±à¸š reward scaling
        "print_verbosity": 1     # à¹€à¸à¸´à¹ˆà¸¡à¸à¸²à¸£à¹à¸ªà¸”à¸‡à¸œà¸¥à¸‚à¹‰à¸­à¸¡à¸¹à¸¥
    }
    
    train_env = StockTradingEnv(df=train_df, **env_kwargs)
    test_env = StockTradingEnv(df=test_df, **env_kwargs)
    
    print("âœ… Environment created successfully")
    
    return train_env, test_env, train_df, test_df

def train_agent(train_env):
    """
    à¹€à¸—à¸£à¸™ DRL Agent à¸”à¹‰à¸§à¸¢à¸à¸²à¸£à¸›à¸£à¸±à¸šà¹à¸•à¹ˆà¸‡ hyperparameters
    """
    print("ğŸ¤– Training DRL Agent...")
    
    # à¸ªà¸£à¹‰à¸²à¸‡ agent
    agent = DRLAgent(env=train_env)
    
    # à¸›à¸£à¸±à¸šà¹à¸•à¹ˆà¸‡ hyperparameters à¹ƒà¸«à¹‰à¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡à¸à¸±à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥ crypto
    PPO_PARAMS = {
        'learning_rate': 1e-4,      # à¸¥à¸” learning rate à¸¥à¸‡
        'n_steps': 1024,           # à¸¥à¸”à¸ˆà¸³à¸™à¸§à¸™ steps à¸•à¹ˆà¸­ batch
        'batch_size': 128,         # à¹€à¸à¸´à¹ˆà¸¡ batch size
        'n_epochs': 4,             # à¸¥à¸”à¸ˆà¸³à¸™à¸§à¸™ epochs
        'gamma': 0.99,             # discount factor
        'gae_lambda': 0.95,        # GAE parameter
        'clip_range': 0.2,         # PPO clip range
        'max_grad_norm': 0.5,      # gradient clipping
        'ent_coef': 0.01,          # entropy coefficient
        'vf_coef': 0.5,            # value function coefficient
        'target_kl': 0.02          # target KL divergence
    }
    
    # à¹€à¸¥à¸·à¸­à¸ model (PPO à¸—à¸µà¹ˆà¸›à¸£à¸±à¸šà¹à¸•à¹ˆà¸‡à¹à¸¥à¹‰à¸§)
    model_name = "ppo"
    print(f"ğŸ§  Using {model_name.upper()} model")
    print(f"Model parameters: {PPO_PARAMS}")
    
    try:
        model = agent.get_model(model_name, model_kwargs=PPO_PARAMS)
        
        # à¹€à¸—à¸£à¸™ model
        print("â³ Training started... (this may take a few minutes)")
        trained_model = agent.train_model(
            model=model,
            tb_log_name=f"minimal_crypto_{model_name}",
            total_timesteps=100000  # à¹€à¸à¸´à¹ˆà¸¡à¸ˆà¸³à¸™à¸§à¸™ timesteps
        )
        
        # à¸šà¸±à¸™à¸—à¸¶à¸ model
        if not os.path.exists(MODEL_DIR):
            os.makedirs(MODEL_DIR)
        model_path = os.path.join(MODEL_DIR, f"minimal_crypto_{model_name}")
        trained_model.save(model_path)
        print(f"ğŸ’¾ Model saved to {model_path}")
        
        return trained_model
        
    except Exception as e:
        print(f"âŒ Error during model training: {str(e)}")
        print("ğŸ’¡ Trying with simplified parameters...")
        
        # à¸¥à¸­à¸‡à¹ƒà¸Šà¹‰ parameters à¸—à¸µà¹ˆà¹€à¸£à¸µà¸¢à¸šà¸‡à¹ˆà¸²à¸¢à¸‚à¸¶à¹‰à¸™
        SIMPLE_PARAMS = {
            'learning_rate': 1e-4,
            'batch_size': 128,
            'n_steps': 1024,
            'gamma': 0.99,
            'gae_lambda': 0.95
        }
        
        print(f"New parameters: {SIMPLE_PARAMS}")
        model = agent.get_model(model_name, model_kwargs=SIMPLE_PARAMS)
        
        trained_model = agent.train_model(
            model=model,
            tb_log_name=f"minimal_crypto_{model_name}_simple",
            total_timesteps=100000
        )
        
        if not os.path.exists(MODEL_DIR):
            os.makedirs(MODEL_DIR)
        model_path = os.path.join(MODEL_DIR, f"minimal_crypto_{model_name}_simple")
        trained_model.save(model_path)
        print(f"ğŸ’¾ Model saved to {model_path}")
        
        return trained_model

def test_agent(trained_model, test_env):
    """
    à¸—à¸”à¸ªà¸­à¸š agent à¸—à¸µà¹ˆà¹€à¸—à¸£à¸™à¹à¸¥à¹‰à¸§
    """
    print("ğŸ“Š Testing trained agent...")
    
    # à¸£à¸±à¸™ backtest
    df_account_value, df_actions = DRLAgent.DRL_prediction(
        model=trained_model,
        environment=test_env
    )
    
    print("âœ… Backtesting completed")
    
    return df_account_value, df_actions

def analyze_results(df_account_value, test_df):
    """
    à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œ
    """
    print("ğŸ“ˆ Analyzing results...")
    
    # à¸„à¸³à¸™à¸§à¸“ returns
    initial_value = INITIAL_AMOUNT
    final_value = df_account_value['account_value'].iloc[-1]
    total_return = (final_value - initial_value) / initial_value * 100
    
    # à¸„à¸³à¸™à¸§à¸“ buy and hold return (BTC)
    btc_initial = test_df['close'].iloc[0]
    btc_final = test_df['close'].iloc[-1] 
    btc_return = (btc_final - btc_initial) / btc_initial * 100
    
    print(f"\nğŸ“Š RESULTS SUMMARY:")
    print(f"{'='*50}")
    print(f"Initial Portfolio Value: ${initial_value:,.2f}")
    print(f"Final Portfolio Value: ${final_value:,.2f}")
    print(f"Agent Total Return: {total_return:.2f}%")
    print(f"BTC Buy & Hold Return: {btc_return:.2f}%")
    print(f"Alpha (Agent - B&H): {total_return - btc_return:.2f}%")
    print(f"{'='*50}")
    
    # Plot results
    plot_results(df_account_value, test_df)
    
    return {
        'agent_return': total_return,
        'btc_return': btc_return,
        'alpha': total_return - btc_return,
        'final_value': final_value
    }

def plot_results(df_account_value, test_df):
    """
    à¸ªà¸£à¹‰à¸²à¸‡à¸à¸£à¸²à¸Ÿà¹à¸ªà¸”à¸‡à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œ
    """
    print("ğŸ“Š Creating performance plots...")
    
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))
    
    # Plot 1: Portfolio value over time
    dates = pd.to_datetime(test_df['timestamp'].unique())
    portfolio_values = df_account_value['account_value'].values
    
    ax1.plot(dates, portfolio_values, label='Agent Portfolio', linewidth=2, color='blue')
    ax1.axhline(y=INITIAL_AMOUNT, color='red', linestyle='--', label='Initial Value')
    ax1.set_title('Portfolio Value Over Time', fontsize=14, fontweight='bold')
    ax1.set_ylabel('Portfolio Value ($)')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Plot 2: BTC Price comparison
    btc_prices = test_df.groupby('timestamp')['close'].first().values
    btc_normalized = btc_prices / btc_prices[0] * INITIAL_AMOUNT
    
    portfolio_normalized = portfolio_values
    
    ax2.plot(dates, portfolio_normalized, label='Agent Portfolio', linewidth=2, color='blue')
    ax2.plot(dates, btc_normalized, label='BTC Buy & Hold', linewidth=2, color='orange')
    ax2.set_title('Agent vs Buy & Hold Comparison', fontsize=14, fontweight='bold')
    ax2.set_ylabel('Normalized Value ($)')
    ax2.set_xlabel('Date')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(MODEL_DIR, 'performance_analysis.png'), dpi=300, bbox_inches='tight')
    plt.show()
    
    print("âœ… Performance plots saved and displayed")

def main():
    """
    Main function - à¸£à¸±à¸™ minimal crypto agent
    """
    print("ğŸš€ Starting Minimal Crypto Agent with FinRL")
    print("="*60)
    
    try:
        # Step 1: Download data (à¹ƒà¸ªà¹ˆ force_download=True à¸–à¹‰à¸²à¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸”à¸²à¸§à¸™à¹Œà¹‚à¸«à¸¥à¸”à¹ƒà¸«à¸¡à¹ˆ)
        df = download_crypto_data(force_download=False)
        
        # Step 2: Add technical indicators  
        df = add_technical_indicators(df)
        
        # Step 3: Create environments
        train_env, test_env, train_df, test_df = create_environment(df)
        
        # Step 4: Train agent
        trained_model = train_agent(train_env)
        
        # Step 5: Test agent
        df_account_value, df_actions = test_agent(trained_model, test_env)
        
        # Step 6: Analyze results
        results = analyze_results(df_account_value, test_df)
        
        print("\nğŸ‰ Minimal Crypto Agent completed successfully!")
        print(f"ğŸ† Your agent achieved {results['agent_return']:.2f}% return")
        
        if results['alpha'] > 0:
            print(f"ğŸ¯ Great! Agent outperformed Buy & Hold by {results['alpha']:.2f}%")
        else:
            print(f"ğŸ“ˆ Agent underperformed Buy & Hold by {abs(results['alpha']):.2f}%")
            print("ğŸ’¡ Try adjusting parameters or training longer!")
            
    except Exception as e:
        print(f"âŒ Error occurred: {str(e)}")
        print("ğŸ’¡ Check your internet connection and try again")

if __name__ == "__main__":
    main()
    