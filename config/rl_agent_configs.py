# rl_agent_configs.py - Optimized RL Agent Configurations for Different Grades
"""
Reinforcement Learning Agent Configurations ‡∏ó‡∏µ‡πà optimize ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏∞‡∏î‡∏±‡∏ö Agent Grades ‡∏ï‡πà‡∏≤‡∏á‡πÜ
‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Grade System: N, D, C, B, A, S (‡∏à‡∏≤‡∏Å‡∏ï‡πà‡∏≥‡πÑ‡∏õ‡∏™‡∏π‡∏á)

‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö RL Algorithms:
- SAC (Soft Actor-Critic) - Off-policy, continuous actions
- PPO (Proximal Policy Optimization) - On-policy, stable
- DQN (Deep Q-Network) - Off-policy, discrete actions  
- DDPG (Deep Deterministic Policy Gradient) - Off-policy, continuous actions
- A2C (Advantage Actor-Critic) - On-policy, fast
- TD3 (Twin Delayed DDPG) - Off-policy, improved DDPG

Grade ‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á:
- N (Novice): ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô, ‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ô‡πâ‡∏≠‡∏¢, simple strategies
- D (Developing): ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏û‡∏±‡∏í‡∏ô‡∏≤, moderate resources
- C (Competent): ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏µ, standard resources  
- B (Proficient): ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç, high resources
- A (Advanced): ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á, very high resources
- S (Supreme): ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î, maximum resources, complex strategies
"""

import os
import numpy as np

# Import config ‡∏´‡∏•‡∏±‡∏Å
from config import *

def linear_schedule(initial_value, final_value=None):
    """
    Linear learning rate schedule
    
    Args:
        initial_value (float): Initial learning rate
        final_value (float, optional): Final learning rate. If None, will use initial_value
        
    Returns:
        function: Scheduler function
    """
    if final_value is None:
        final_value = initial_value
        
    def scheduler(progress_remaining):
        """
        Progress will go from 1 (beginning) to 0 (end)
        """
        return final_value + progress_remaining * (initial_value - final_value)
    
    return scheduler

class BaseGradeConfigs:
    """Base class ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RL Agent Configurations"""
    
    @staticmethod
    def get_common_config(algorithm: str):
        """Common configuration ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å algorithms"""
        return {
            'policy': 'MlpPolicy',
            'verbose': 1,
            'device': 'auto',
            'seed': 42,
            'tensorboard_log': f'./logs/{algorithm.lower()}_graded/',
        }

class SAC_GradeConfigs(BaseGradeConfigs):
    """SAC Configurations ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ Grade Level"""
    
    @staticmethod
    def get_base_config():
        """Base configuration ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö SAC"""
        config = BaseGradeConfigs.get_common_config('SAC')
        config.update({
            'algorithm': 'SAC',
            'action_space_type': 'continuous'
        })
        return config
    
    @staticmethod
    def get_grade_N_config():
        """
        Grade N (Novice) - ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
        - ‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ô‡πâ‡∏≠‡∏¢, training ‡πÄ‡∏£‡πá‡∏ß
        - ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ
        - Stable ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô
        """
        config = SAC_GradeConfigs.get_base_config()
        config.update({
            'learning_rate': 3e-4,              # Standard learning rate
            'buffer_size': 50000,               # Small buffer ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö memory
            'learning_starts': 1000,            # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏£‡πá‡∏ß
            'batch_size': 128,                  # Small batch
            'tau': 0.005,                       # Standard target update
            'gamma': 0.99,                      # Standard discount
            'train_freq': 1,                    # ‡πÄ‡∏ó‡∏£‡∏ô‡∏ö‡πà‡∏≠‡∏¢
            'gradient_steps': 1,                # Simple gradient steps
            'target_update_interval': 1,
            'ent_coef': 0.2,                   # Fixed entropy (stable)
            'target_entropy': 'auto',
            'use_sde': False,                   # ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ SDE (simple)
            'total_timesteps': 50000,           # Training ‡∏™‡∏±‡πâ‡∏ô
            'eval_freq': 5000,                  # Evaluate ‡∏ö‡πà‡∏≠‡∏¢
            'grade': 'N',
            'description': 'Novice: Fast training, stable but simple'
        })
        return config
    
    @staticmethod
    def get_grade_D_config():
        """
        Grade D (Developing) - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏û‡∏±‡∏í‡∏ô‡∏≤
        - ‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á
        - ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÉ‡∏ä‡πâ advanced features
        - Balance ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á speed ‡πÅ‡∏•‡∏∞ performance
        """
        config = SAC_GradeConfigs.get_base_config()
        config.update({
            'learning_rate': 1e-4,              # ‡∏•‡∏î‡∏•‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠ stability
            'buffer_size': 100000,              # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏ô‡∏≤‡∏î buffer
            'learning_starts': 2000,            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô
            'batch_size': 256,                  # Larger batch
            'tau': 0.01,                        # Faster target update
            'gamma': 0.995,                     # Long-term thinking
            'train_freq': 2,                    # ‡∏•‡∏î frequency ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
            'gradient_steps': 2,                # Multiple gradient steps
            'target_update_interval': 1,
            'ent_coef': 'auto',                # Automatic entropy tuning
            'target_entropy': 'auto',
            'use_sde': False,                   # ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ SDE
            'total_timesteps': 100000,          # Training ‡∏ô‡∏≤‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô
            'eval_freq': 10000,
            'grade': 'D',
            'description': 'Developing: Balanced speed and performance'
        })
        return config
    
    @staticmethod
    def get_grade_C_config():
        """
        Grade C (Competent) - ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏µ
        - Standard resources
        - ‡πÉ‡∏ä‡πâ advanced features ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô
        - Good performance baseline
        """
        config = SAC_GradeConfigs.get_base_config()
        config.update({
            'learning_rate': linear_schedule(1e-4, 3e-5),  # Learning rate decay
            'buffer_size': 250000,              # Larger buffer
            'learning_starts': 5000,            # More warm-up
            'batch_size': 512,                  # Large batch
            'tau': 0.01,                        # Fast target update
            'gamma': 0.995,                     # Long-term focus
            'train_freq': 4,                    # Less frequent training
            'gradient_steps': 4,                # Multiple gradient steps
            'target_update_interval': 1,
            'ent_coef': 'auto',                # Auto entropy
            'target_entropy': 'auto',
            'use_sde': True,                    # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÉ‡∏ä‡πâ SDE
            'sde_sample_freq': 64,              # SDE sampling
            'total_timesteps': 200000,          # Longer training
            'eval_freq': 15000,
            'grade': 'C',
            'description': 'Competent: Good baseline performance'
        })
        return config
    
    @staticmethod
    def get_grade_B_config():
        """
        Grade B (Proficient) - ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç
        - High resources
        - Advanced optimization techniques
        - Focus on performance
        """
        config = SAC_GradeConfigs.get_base_config()
        config.update({
            'learning_rate': linear_schedule(3e-4, 1e-5),  # More aggressive decay
            'buffer_size': 500000,              # Large buffer
            'learning_starts': 10000,           # Extensive warm-up
            'batch_size': 1024,                 # Very large batch
            'tau': 0.02,                        # Faster target update
            'gamma': 0.999,                     # Very long-term
            'train_freq': 8,                    # Less frequent but intensive
            'gradient_steps': 8,                # Many gradient steps
            'target_update_interval': 1,
            'ent_coef': 'auto',                # Auto entropy
            'target_entropy': 'auto',
            'use_sde': True,                    # Advanced exploration
            'sde_sample_freq': 32,              # More frequent SDE
            'total_timesteps': 500000,          # Long training
            'eval_freq': 25000,
            'grade': 'B',
            'description': 'Proficient: High performance focus'
        })
        return config
    
    @staticmethod
    def get_grade_A_config():
        """
        Grade A (Advanced) - ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á
        - Very high resources
        - Advanced techniques + ensemble ready
        - High performance requirements
        """
        config = SAC_GradeConfigs.get_base_config()
        config.update({
            'learning_rate': linear_schedule(3e-4, 5e-6),  # Extended decay
            'buffer_size': 1000000,             # Very large buffer
            'learning_starts': 20000,           # Extensive warm-up
            'batch_size': 2048,                 # Massive batch
            'tau': 0.02,                        # Fast target update
            'gamma': 0.999,                     # Maximum long-term
            'train_freq': 16,                   # Intensive training
            'gradient_steps': 16,               # Many gradient steps
            'target_update_interval': 1,
            'ent_coef': 'auto',                # Auto entropy
            'target_entropy': 'auto',
            'use_sde': True,                    # Advanced exploration
            'sde_sample_freq': 16,              # High frequency SDE
            'total_timesteps': 1000000,         # Very long training
            'eval_freq': 50000,
            'policy_kwargs': {                  # Advanced policy network
                'net_arch': [512, 512, 256],
                'activation_fn': 'relu'
            },
            'grade': 'A',
            'description': 'Advanced: Very high performance, ensemble ready'
        })
        return config
    
    @staticmethod
    def get_grade_S_config():
        """
        Grade S (Supreme) - ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
        - Maximum resources
        - All advanced techniques
        - Supreme performance
        - Research-grade configuration
        """
        config = SAC_GradeConfigs.get_base_config()
        config.update({
            'learning_rate': linear_schedule(5e-4, 1e-6),  # Complex decay
            'buffer_size': 2000000,             # Maximum buffer
            'learning_starts': 50000,           # Massive warm-up
            'batch_size': 4096,                 # Maximum batch
            'tau': 0.025,                       # Aggressive target update
            'gamma': 0.9995,                    # Supreme long-term
            'train_freq': 32,                   # Maximum intensive training
            'gradient_steps': 32,               # Maximum gradient steps
            'target_update_interval': 1,
            'ent_coef': 'auto',                # Auto entropy
            'target_entropy': 'auto',
            'use_sde': True,                    # Maximum exploration
            'sde_sample_freq': 8,               # Maximum frequency SDE
            'total_timesteps': 2000000,         # Supreme training
            'eval_freq': 100000,
            'policy_kwargs': {                  # Supreme policy network
                'net_arch': [1024, 1024, 512, 256],
                'activation_fn': 'relu',
                'dropout': 0.1
            },
            'grade': 'S',
            'description': 'Supreme: Maximum performance, research-grade'
        })
        return config

class PPO_GradeConfigs(BaseGradeConfigs):
    """PPO Configurations ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ Grade Level"""
    
    @staticmethod
    def get_base_config():
        """Base configuration ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö PPO"""
        config = BaseGradeConfigs.get_common_config('PPO')
        config.update({
            'algorithm': 'PPO',
            'action_space_type': 'both'
        })
        return config
    
    @staticmethod
    def get_grade_N_config():
        """Grade N (Novice) - PPO"""
        config = PPO_GradeConfigs.get_base_config()
        config.update({
            'learning_rate': 3e-4,
            'n_steps': 1024,
            'batch_size': 128,
            'n_epochs': 4,
            'gamma': 0.99,
            'gae_lambda': 0.95,
            'clip_range': 0.2,
            'ent_coef': 0.01,
            'vf_coef': 0.5,
            'max_grad_norm': 0.5,
            'total_timesteps': 50000,
            'eval_freq': 5000,
            'grade': 'N',
            'description': 'PPO Novice: Simple and stable'
        })
        return config
    
    @staticmethod
    def get_grade_D_config():
        """Grade D (Developing) - PPO"""
        config = PPO_GradeConfigs.get_base_config()
        config.update({
            'learning_rate': linear_schedule(3e-4, 1e-4),
            'n_steps': 2048,
            'batch_size': 256,
            'n_epochs': 8,
            'gamma': 0.995,
            'gae_lambda': 0.95,
            'clip_range': 0.2,
            'ent_coef': 0.01,
            'vf_coef': 0.5,
            'max_grad_norm': 0.5,
            'total_timesteps': 100000,
            'eval_freq': 10000,
            'grade': 'D',
            'description': 'PPO Developing: Balanced performance'
        })
        return config
    
    @staticmethod
    def get_grade_C_config():
        """Grade C (Competent) - PPO"""
        config = PPO_GradeConfigs.get_base_config()
        config.update({
            'learning_rate': linear_schedule(3e-4, 5e-5),
            'n_steps': 4096,
            'batch_size': 512,
            'n_epochs': 10,
            'gamma': 0.995,
            'gae_lambda': 0.98,
            'clip_range': linear_schedule(0.2, 0.1),
            'ent_coef': 0.005,
            'vf_coef': 0.5,
            'max_grad_norm': 0.5,
            'total_timesteps': 200000,
            'eval_freq': 15000,
            'grade': 'C',
            'description': 'PPO Competent: Good baseline'
        })
        return config
    
    @staticmethod
    def get_grade_B_config():
        """Grade B (Proficient) - PPO"""
        config = PPO_GradeConfigs.get_base_config()
        config.update({
            'learning_rate': linear_schedule(3e-4, 1e-5),
            'n_steps': 8192,
            'batch_size': 1024,
            'n_epochs': 15,
            'gamma': 0.999,
            'gae_lambda': 0.98,
            'clip_range': linear_schedule(0.2, 0.05),
            'ent_coef': 0.001,
            'vf_coef': 0.5,
            'max_grad_norm': 0.5,
            'total_timesteps': 500000,
            'eval_freq': 25000,
            'grade': 'B',
            'description': 'PPO Proficient: High performance'
        })
        return config
    
    @staticmethod
    def get_grade_A_config():
        """Grade A (Advanced) - PPO"""
        config = PPO_GradeConfigs.get_base_config()
        config.update({
            'learning_rate': linear_schedule(3e-4, 5e-6),
            'n_steps': 16384,
            'batch_size': 2048,
            'n_epochs': 20,
            'gamma': 0.999,
            'gae_lambda': 0.99,
            'clip_range': linear_schedule(0.2, 0.02),
            'ent_coef': 0.0005,
            'vf_coef': 0.5,
            'max_grad_norm': 0.5,
            'policy_kwargs': {
                'net_arch': [512, 512, 256],
                'activation_fn': 'relu'
            },
            'total_timesteps': 1000000,
            'eval_freq': 50000,
            'grade': 'A',
            'description': 'PPO Advanced: Very high performance'
        })
        return config
    
    @staticmethod
    def get_grade_S_config():
        """Grade S (Supreme) - PPO"""
        config = PPO_GradeConfigs.get_base_config()
        config.update({
            'learning_rate': linear_schedule(5e-4, 1e-6),
            'n_steps': 32768,
            'batch_size': 4096,
            'n_epochs': 30,
            'gamma': 0.9995,
            'gae_lambda': 0.99,
            'clip_range': linear_schedule(0.3, 0.01),
            'ent_coef': 0.0001,
            'vf_coef': 0.5,
            'max_grad_norm': 0.5,
            'policy_kwargs': {
                'net_arch': [1024, 1024, 512, 256],
                'activation_fn': 'relu',
                'dropout': 0.1
            },
            'total_timesteps': 2000000,
            'eval_freq': 100000,
            'grade': 'S',
            'description': 'PPO Supreme: Maximum performance'
        })
        return config

class DQN_GradeConfigs(BaseGradeConfigs):
    """DQN Configurations ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ Grade Level"""
    
    @staticmethod
    def get_base_config():
        """Base configuration ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö DQN"""
        config = BaseGradeConfigs.get_common_config('DQN')
        config.update({
            'algorithm': 'DQN',
            'action_space_type': 'discrete'
        })
        return config
    
    @staticmethod
    def get_grade_N_config():
        """Grade N (Novice) - DQN"""
        config = DQN_GradeConfigs.get_base_config()
        config.update({
            'learning_rate': 1e-4,
            'buffer_size': 50000,
            'learning_starts': 1000,
            'batch_size': 32,
            'tau': 1.0,
            'gamma': 0.99,
            'train_freq': 4,
            'gradient_steps': 1,
            'target_update_interval': 1000,
            'exploration_fraction': 0.3,
            'exploration_initial_eps': 1.0,
            'exploration_final_eps': 0.05,
            'total_timesteps': 50000,
            'eval_freq': 5000,
            'grade': 'N',
            'description': 'DQN Novice: Simple discrete actions'
        })
        return config
    
    @staticmethod
    def get_grade_D_config():
        """Grade D (Developing) - DQN"""
        config = DQN_GradeConfigs.get_base_config()
        config.update({
            'learning_rate': 5e-5,
            'buffer_size': 100000,
            'learning_starts': 2000,
            'batch_size': 64,
            'tau': 1.0,
            'gamma': 0.995,
            'train_freq': 8,
            'gradient_steps': 2,
            'target_update_interval': 2000,
            'exploration_fraction': 0.4,
            'exploration_initial_eps': 1.0,
            'exploration_final_eps': 0.02,
            'total_timesteps': 100000,
            'eval_freq': 10000,
            'grade': 'D',
            'description': 'DQN Developing: Improved exploration'
        })
        return config
    
    @staticmethod
    def get_grade_C_config():
        """Grade C (Competent) - DQN"""
        config = DQN_GradeConfigs.get_base_config()
        config.update({
            'learning_rate': linear_schedule(1e-4, 1e-5),
            'buffer_size': 250000,
            'learning_starts': 5000,
            'batch_size': 128,
            'tau': 1.0,
            'gamma': 0.995,
            'train_freq': 16,
            'gradient_steps': 4,
            'target_update_interval': 5000,
            'exploration_fraction': 0.5,
            'exploration_initial_eps': 1.0,
            'exploration_final_eps': 0.01,
            'total_timesteps': 200000,
            'eval_freq': 15000,
            'grade': 'C',
            'description': 'DQN Competent: Advanced exploration'
        })
        return config

class DDPG_GradeConfigs(BaseGradeConfigs):
    """DDPG Configurations ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ Grade Level"""
    
    @staticmethod
    def get_base_config():
        """Base configuration ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö DDPG"""
        config = BaseGradeConfigs.get_common_config('DDPG')
        config.update({
            'algorithm': 'DDPG',
            'action_space_type': 'continuous'
        })
        return config
    
    @staticmethod
    def get_grade_N_config():
        """Grade N (Novice) - DDPG"""
        config = DDPG_GradeConfigs.get_base_config()
        config.update({
            'learning_rate': 1e-4,
            'buffer_size': 50000,
            'learning_starts': 1000,
            'batch_size': 128,
            'tau': 0.005,
            'gamma': 0.99,
            'train_freq': 1,
            'gradient_steps': 1,
            'action_noise': 'normal',
            'total_timesteps': 50000,
            'eval_freq': 5000,
            'grade': 'N',
            'description': 'DDPG Novice: Basic continuous control'
        })
        return config

class TD3_GradeConfigs(BaseGradeConfigs):
    """TD3 Configurations ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ Grade Level"""
    
    @staticmethod
    def get_base_config():
        """Base configuration ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö TD3"""
        config = BaseGradeConfigs.get_common_config('TD3')
        config.update({
            'algorithm': 'TD3',
            'action_space_type': 'continuous'
        })
        return config
    
    @staticmethod
    def get_grade_N_config():
        """Grade N (Novice) - TD3"""
        config = TD3_GradeConfigs.get_base_config()
        config.update({
            'learning_rate': 1e-4,
            'buffer_size': 50000,
            'learning_starts': 1000,
            'batch_size': 128,
            'tau': 0.005,
            'gamma': 0.99,
            'train_freq': 1,
            'gradient_steps': 1,
            'policy_delay': 2,
            'target_policy_noise': 0.2,
            'target_noise_clip': 0.5,
            'total_timesteps': 50000,
            'eval_freq': 5000,
            'grade': 'N',
            'description': 'TD3 Novice: Improved DDPG'
        })
        return config

class A2C_GradeConfigs(BaseGradeConfigs):
    """A2C Configurations ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ Grade Level"""
    
    @staticmethod
    def get_base_config():
        """Base configuration ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö A2C"""
        config = BaseGradeConfigs.get_common_config('A2C')
        config.update({
            'algorithm': 'A2C',
            'action_space_type': 'both'
        })
        return config
    
    @staticmethod
    def get_grade_N_config():
        """Grade N (Novice) - A2C"""
        config = A2C_GradeConfigs.get_base_config()
        config.update({
            'learning_rate': 7e-4,
            'n_steps': 5,
            'gamma': 0.99,
            'gae_lambda': 1.0,
            'ent_coef': 0.01,
            'vf_coef': 0.25,
            'max_grad_norm': 0.5,
            'rms_prop_eps': 1e-5,
            'use_rms_prop': True,
            'total_timesteps': 50000,
            'eval_freq': 5000,
            'grade': 'N',
            'description': 'A2C Novice: Fast and simple'
        })
        return config

class RL_GradeSelector:
    """Universal RL Agent Selector ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å algorithms"""
    
    # Algorithm mappings
    ALGORITHM_CONFIGS = {
        'SAC': SAC_GradeConfigs,
        'PPO': PPO_GradeConfigs,
        'DQN': DQN_GradeConfigs,
        'DDPG': DDPG_GradeConfigs,
        'TD3': TD3_GradeConfigs,
        'A2C': A2C_GradeConfigs
    }
    
    @staticmethod
    def get_available_algorithms():
        """‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ algorithms ‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö"""
        return list(RL_GradeSelector.ALGORITHM_CONFIGS.keys())
    
    @staticmethod
    def get_config_by_algorithm_and_grade(algorithm: str, grade: str):
        """
        ‡∏£‡∏±‡∏ö config ‡∏ï‡∏≤‡∏° algorithm ‡πÅ‡∏•‡∏∞ grade
        
        Args:
            algorithm (str): Algorithm name ('SAC', 'PPO', 'DQN', etc.)
            grade (str): Grade level ('N', 'D', 'C', 'B', 'A', 'S')
            
        Returns:
            dict: RL Agent configuration
        """
        algorithm = algorithm.upper()
        grade = grade.upper()
        
        if algorithm not in RL_GradeSelector.ALGORITHM_CONFIGS:
            raise ValueError(f"Unsupported algorithm: {algorithm}. "
                           f"Available: {RL_GradeSelector.get_available_algorithms()}")
        
        config_class = RL_GradeSelector.ALGORITHM_CONFIGS[algorithm]
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ grade method ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        grade_method = f'get_grade_{grade}_config'
        if not hasattr(config_class, grade_method):
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ grade ‡∏ô‡∏±‡πâ‡∏ô ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ grade N ‡πÅ‡∏ó‡∏ô
            if hasattr(config_class, 'get_grade_N_config'):
                print(f"‚ö†Ô∏è Grade {grade} not available for {algorithm}, using Grade N instead")
                grade_method = 'get_grade_N_config'
            else:
                raise ValueError(f"No configurations available for {algorithm}")
        
        return getattr(config_class, grade_method)()
    
    @staticmethod
    def get_recommended_algorithm_for_crypto(action_space_type: str = 'continuous'):
        """
        ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ algorithm ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö cryptocurrency trading
        
        Args:
            action_space_type (str): 'continuous', 'discrete', or 'both'
            
        Returns:
            list: Recommended algorithms in order of preference
        """
        if action_space_type == 'continuous':
            return ['SAC', 'TD3', 'DDPG', 'PPO']
        elif action_space_type == 'discrete':
            return ['DQN', 'PPO', 'A2C']
        else:  # both
            return ['PPO', 'SAC', 'A2C', 'TD3']
    
    @staticmethod
    def get_config_by_performance(algorithm: str, target_return: float, available_time_hours: int = 24):
        """
        ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å config ‡∏ï‡∏≤‡∏° algorithm, target performance ‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ
        
        Args:
            algorithm (str): Algorithm name
            target_return (float): Target return (%)
            available_time_hours (int): Available training time (hours)
            
        Returns:
            dict: Recommended RL Agent configuration
        """
        if target_return <= 5 or available_time_hours <= 2:
            grade = 'N'
        elif target_return <= 10 or available_time_hours <= 6:
            grade = 'D'
        elif target_return <= 15 or available_time_hours <= 12:
            grade = 'C'
        elif target_return <= 25 or available_time_hours <= 24:
            grade = 'B'
        elif target_return <= 40 or available_time_hours <= 48:
            grade = 'A'
        else:
            grade = 'S'
        
        return RL_GradeSelector.get_config_by_algorithm_and_grade(algorithm, grade)
    
    @staticmethod
    def get_config_by_resources(algorithm: str, ram_gb: int, gpu_available: bool = False):
        """
        ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å config ‡∏ï‡∏≤‡∏° algorithm ‡πÅ‡∏•‡∏∞‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ
        
        Args:
            algorithm (str): Algorithm name
            ram_gb (int): Available RAM in GB
            gpu_available (bool): GPU availability
            
        Returns:
            dict: Recommended RL Agent configuration
        """
        if ram_gb <= 8:
            grade = 'N'
        elif ram_gb <= 16:
            grade = 'D'
        elif ram_gb <= 32:
            grade = 'C'
        elif ram_gb <= 64 and not gpu_available:
            grade = 'B'
        elif ram_gb <= 64 and gpu_available:
            grade = 'A'
        else:  # ram_gb > 64 and gpu_available
            grade = 'S'
        
        return RL_GradeSelector.get_config_by_algorithm_and_grade(algorithm, grade)

# Legacy support - ‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠ backward compatibility
class SAC_GradeSelector:
    """Legacy SAC Selector - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö backward compatibility"""
    
    @staticmethod
    def get_config_by_grade(grade: str):
        """Legacy method - ‡πÉ‡∏ä‡πâ SAC ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô"""
        return RL_GradeSelector.get_config_by_algorithm_and_grade('SAC', grade)
    
    @staticmethod
    def get_config_by_performance(target_return: float, available_time_hours: int = 24):
        """Legacy method - ‡πÉ‡∏ä‡πâ SAC ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô"""
        return RL_GradeSelector.get_config_by_performance('SAC', target_return, available_time_hours)
    
    @staticmethod
    def get_config_by_resources(ram_gb: int, gpu_available: bool = False):
        """Legacy method - ‡πÉ‡∏ä‡πâ SAC ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô"""
        return RL_GradeSelector.get_config_by_resources('SAC', ram_gb, gpu_available)

def print_algorithm_comparison():
    """‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö algorithms ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"""
    algorithms = RL_GradeSelector.get_available_algorithms()
    
    print("ü§ñ RL Algorithms Comparison")
    print("=" * 80)
    
    for algorithm in algorithms:
        try:
            config = RL_GradeSelector.get_config_by_algorithm_and_grade(algorithm, 'N')
            action_space = config.get('action_space_type', 'unknown')
            description = config.get('description', 'No description')
            
            print(f"\n{algorithm}:")
            print(f"  Action Space: {action_space}")
            print(f"  Description: {description}")
        except Exception as e:
            print(f"\n{algorithm}: Configuration not available")

def print_grade_comparison():
    """‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö configs ‡∏ó‡∏∏‡∏Å grades ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö SAC"""
    grades = ['N', 'D', 'C', 'B', 'A', 'S']
    
    print("üéØ SAC Configuration Comparison by Grade")
    print("=" * 80)
    
    configs = []
    for grade in grades:
        try:
            config = RL_GradeSelector.get_config_by_algorithm_and_grade('SAC', grade)
            configs.append(config)
        except:
            configs.append(None)
    
    # Key parameters to compare
    key_params = [
        'buffer_size', 'total_timesteps', 'batch_size', 
        'gradient_steps', 'learning_starts'
    ]
    
    print(f"{'Grade':<6} {'Buffer':<10} {'Timesteps':<12} {'Batch':<8} {'Grad Steps':<12} {'Learning Starts':<15}")
    print("-" * 80)
    
    for i, grade in enumerate(grades):
        config = configs[i]
        if config:
            print(f"{grade:<6} {config.get('buffer_size', 'N/A'):<10} {config.get('total_timesteps', 'N/A'):<12} "
                  f"{config.get('batch_size', 'N/A'):<8} {config.get('gradient_steps', 'N/A'):<12} {config.get('learning_starts', 'N/A'):<15}")
        else:
            print(f"{grade:<6} {'N/A':<10} {'N/A':<12} {'N/A':<8} {'N/A':<12} {'N/A':<15}")
    
    print("\nüìù Grade Descriptions:")
    print("-" * 40)
    for i, grade in enumerate(grades):
        config = configs[i]
        if config:
            print(f"{grade}: {config.get('description', 'No description')}")

def get_recommended_grade_for_crypto():
    """
    ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ grade ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö cryptocurrency trading
    ‡∏ï‡∏≤‡∏° market characteristics
    """
    print("ü™ô Recommended Grades for Cryptocurrency Trading")
    print("=" * 60)
    
    recommendations = {
        'Beginner Trader': {
            'algorithm': 'PPO',
            'grade': 'D',
            'reason': 'Stable and easy to understand',
            'suitable_for': 'Learning, small portfolios'
        },
        'Active Trader': {
            'algorithm': 'SAC',
            'grade': 'B',
            'reason': 'Excellent for continuous action spaces',
            'suitable_for': 'Daily trading, medium portfolios'
        },
        'Professional Trader': {
            'algorithm': 'SAC',
            'grade': 'A',
            'reason': 'Advanced performance for professional use',
            'suitable_for': 'Large portfolios, algorithmic trading'
        },
        'Research & Development': {
            'algorithm': 'TD3',
            'grade': 'S',
            'reason': 'State-of-the-art for continuous control',
            'suitable_for': 'Strategy development, academic research'
        }
    }
    
    for trader_type, info in recommendations.items():
        print(f"\n{trader_type}:")
        print(f"  Recommended Algorithm: {info['algorithm']}")
        print(f"  Recommended Grade: {info['grade']}")
        print(f"  Reason: {info['reason']}")
        print(f"  Suitable for: {info['suitable_for']}")

# Example usage functions
def create_rl_agent_by_algorithm_and_grade(algorithm: str, grade: str, env):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á RL agent ‡∏ï‡∏≤‡∏° algorithm ‡πÅ‡∏•‡∏∞ grade ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
    
    Args:
        algorithm (str): Algorithm name
        grade (str): Grade level
        env: Trading environment
        
    Returns:
        tuple: (agent, config)
    """
    config = RL_GradeSelector.get_config_by_algorithm_and_grade(algorithm, grade)
    
    # Import algorithm class
    algorithm_imports = {
        'SAC': 'from stable_baselines3 import SAC',
        'PPO': 'from stable_baselines3 import PPO',
        'DQN': 'from stable_baselines3 import DQN',
        'DDPG': 'from stable_baselines3 import DDPG',
        'TD3': 'from stable_baselines3 import TD3',
        'A2C': 'from stable_baselines3 import A2C'
    }
    
    if algorithm not in algorithm_imports:
        raise ValueError(f"Unsupported algorithm: {algorithm}")
    
    # ‡πÅ‡∏¢‡∏Å parameters ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö agent
    agent_params = config.copy()
    
    # ‡∏•‡∏ö parameters ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏Ç‡∏≠‡∏á agent
    non_agent_params = ['total_timesteps', 'eval_freq', 'grade', 'description', 'algorithm', 'action_space_type']
    for param in non_agent_params:
        if param in agent_params:
            del agent_params[param]
    
    print(f"ü§ñ Creating {algorithm} Agent - Grade {grade}")
    print(f"üìù Description: {config['description']}")
    print(f"‚è±Ô∏è Training timesteps: {config['total_timesteps']:,}")
    if 'buffer_size' in config:
        print(f"üíæ Buffer size: {config['buffer_size']:,}")
    if 'batch_size' in config:
        print(f"üéØ Batch size: {config['batch_size']}")
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á agent ‡∏ï‡∏≤‡∏° algorithm
    if algorithm == 'SAC':
        from stable_baselines3 import SAC
        agent = SAC(env=env, **agent_params)
    elif algorithm == 'PPO':
        from stable_baselines3 import PPO
        agent = PPO(env=env, **agent_params)
    elif algorithm == 'DQN':
        from stable_baselines3 import DQN
        agent = DQN(env=env, **agent_params)
    elif algorithm == 'DDPG':
        from stable_baselines3 import DDPG
        agent = DDPG(env=env, **agent_params)
    elif algorithm == 'TD3':
        from stable_baselines3 import TD3
        agent = TD3(env=env, **agent_params)
    elif algorithm == 'A2C':
        from stable_baselines3 import A2C
        agent = A2C(env=env, **agent_params)
    else:
        raise ValueError(f"Algorithm {algorithm} not implemented")
    
    return agent, config

# Legacy function - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö backward compatibility
def create_sac_agent_by_grade(grade: str, env):
    """Legacy function - ‡∏™‡∏£‡πâ‡∏≤‡∏á SAC agent ‡∏ï‡∏≤‡∏° grade"""
    return create_rl_agent_by_algorithm_and_grade('SAC', grade, env)

if __name__ == "__main__":
    # ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö algorithms
    print_algorithm_comparison()
    print("\n")
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö SAC grades
    print_grade_comparison()
    print("\n")
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö crypto trading
    get_recommended_grade_for_crypto()
    
    # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
    print(f"\nüîß Example Usage:")
    print("=" * 40)
    
    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏≤‡∏° algorithm ‡πÅ‡∏•‡∏∞ grade
    config_sac_b = RL_GradeSelector.get_config_by_algorithm_and_grade('SAC', 'B')
    print(f"SAC Grade B: {config_sac_b['description']}")
    
    config_ppo_a = RL_GradeSelector.get_config_by_algorithm_and_grade('PPO', 'A')
    print(f"PPO Grade A: {config_ppo_a['description']}")
    
    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏≤‡∏° performance target
    config_perf = RL_GradeSelector.get_config_by_performance('TD3', target_return=15.0, available_time_hours=12)
    print(f"TD3 Performance-based: Grade {config_perf['grade']}")
    
    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏≤‡∏° resources
    config_resource = RL_GradeSelector.get_config_by_resources('DDPG', ram_gb=32, gpu_available=True)
    print(f"DDPG Resource-based: Grade {config_resource['grade']}")
    
    # ‡πÅ‡∏™‡∏î‡∏á recommended algorithms ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö crypto
    print(f"\nü™ô Crypto Trading Recommendations:")
    print(f"Continuous Actions: {RL_GradeSelector.get_recommended_algorithm_for_crypto('continuous')}")
    print(f"Discrete Actions: {RL_GradeSelector.get_recommended_algorithm_for_crypto('discrete')}")
    print(f"Both Action Types: {RL_GradeSelector.get_recommended_algorithm_for_crypto('both')}")
    
    # Legacy compatibility test
    print(f"\nüîÑ Legacy Compatibility Test:")
    legacy_config = SAC_GradeSelector.get_config_by_grade('B')
    print(f"Legacy SAC Grade B: {legacy_config['description']}") 