# improved_sac.py - Enhanced SAC Agent ‡∏î‡πâ‡∏ß‡∏¢ Grade A Configuration
import sys
import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import os
import yfinance as yf
import torch
import pickle
import random
import string

# Stable Baselines3 imports
import gymnasium as gym
from stable_baselines3 import SAC
from stable_baselines3.common.env_checker import check_env
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold

# Import configurations
from config import *
from sac_configs import SAC_GradeConfigs, SAC_GradeSelector

# Import existing CryptoTradingEnv from sac.py
from sac import CryptoTradingEnv, setup_device, load_existing_data, add_technical_indicators, create_environment

def train_improved_sac_agent(train_env, grade='A'):
    """
    ‡∏ù‡∏∂‡∏Å SAC Agent ‡∏î‡πâ‡∏ß‡∏¢ Grade Configuration ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß
    
    Args:
        train_env: Training environment
        grade: Grade level (N, D, C, B, A, S) - default 'A' ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö 48GB+GPU
    """
    print(f"\nü§ñ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ù‡∏∂‡∏Å Enhanced SAC Agent (Grade {grade})...")
    print("-" * 50)
    
    # ‡∏î‡∏∂‡∏á configuration ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö grade ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
    from sac_configs import RL_GradeSelector
    config = RL_GradeSelector.get_config_by_algorithm_and_grade('SAC', grade)
    
    print(f"üìä ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Grade {grade}:")
    print(f"   - Training timesteps: {config['total_timesteps']:,}")
    print(f"   - Buffer size: {config['buffer_size']:,}")
    print(f"   - Learning starts: {config['learning_starts']:,}")
    print(f"   - Batch size: {config['batch_size']}")
    print(f"   - Gradient steps: {config['gradient_steps']}")
    print(f"   - Entropy coefficient: {config['ent_coef']}")
    print(f"   - Use SDE: {config['use_sde']}")
    
    # Wrap environment
    vec_env = DummyVecEnv([lambda: train_env])
    
    # Device setup
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"üîß Device: {device}")
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á SAC model ‡∏î‡πâ‡∏ß‡∏¢ Grade configuration
    model_params = {
        'policy': config['policy'],
        'env': vec_env,
        'learning_rate': config['learning_rate'],
        'buffer_size': config['buffer_size'],
        'learning_starts': config['learning_starts'],
        'batch_size': config['batch_size'],
        'tau': config['tau'],
        'gamma': config['gamma'],
        'train_freq': config['train_freq'],
        'gradient_steps': config['gradient_steps'],
        'target_update_interval': config['target_update_interval'],
        'verbose': config['verbose'],
        'seed': config.get('seed', 42),
        'device': device,
        'tensorboard_log': config.get('tensorboard_log', './logs/sac_improved/')
    }
    
    # ‡πÄ‡∏û‡∏¥‡πà‡∏° entropy tuning
    if config['ent_coef'] == 'auto':
        model_params['ent_coef'] = 'auto'
        model_params['target_entropy'] = 'auto'
    else:
        model_params['ent_coef'] = config['ent_coef']
    
    # ‡πÄ‡∏û‡∏¥‡πà‡∏° SDE ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
    if config.get('use_sde', False):
        model_params['use_sde'] = True
        model_params['sde_sample_freq'] = config.get('sde_sample_freq', 64)
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á model
    model = SAC(**model_params)
    
    print(f"‚úÖ SAC Model ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (Grade {grade})")
    print(f"üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å {config['total_timesteps']:,} timesteps...")
    
    # Setup callbacks
    callbacks = []
    
    # Evaluation callback
    eval_freq = config.get('eval_freq', 10000)
    eval_callback = EvalCallback(
        vec_env, 
        best_model_save_path=f'./models/sac/best_model_grade_{grade}/',
        log_path=f'./logs/sac_eval_grade_{grade}/',
        eval_freq=eval_freq,
        deterministic=True,
        render=False,
        verbose=1
    )
    callbacks.append(eval_callback)
    
    # ‡∏ù‡∏∂‡∏Å model
    start_time = datetime.now()
    
    model.learn(
        total_timesteps=config['total_timesteps'], 
        callback=callbacks,
        progress_bar=True
    )
    
    training_time = datetime.now() - start_time
    print(f"‚úÖ ‡∏ù‡∏∂‡∏Å SAC Agent ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤: {training_time})")
    
    return model, config, training_time

def save_improved_sac_agent(trained_model, config, training_time, train_df, test_df):
    """
    ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Improved SAC Agent ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°
    """
    print(f"\nüíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Enhanced SAC Agent (Grade {config['grade']})...")
    print("-" * 50)
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    random_suffix = ''.join(random.choices(string.ascii_uppercase + string.digits, k=6))
    model_name = f"enhanced_sac_grade_{config['grade']}_{timestamp}_{random_suffix}"
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á directory ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
    os.makedirs("models/sac", exist_ok=True)
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å trained model
    model_zip_path = os.path.join("models", "sac", f"{model_name}.zip")
    trained_model.save(model_zip_path)
    print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å model: {model_zip_path}")
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°
    agent_info = {
        'model_name': model_name,
        'algorithm': 'SAC',
        'grade': config['grade'],
        'grade_description': config['description'],
        'created_date': datetime.now().isoformat(),
        'training_time': str(training_time),
        'crypto_symbols': CRYPTO_SYMBOLS,
        'indicators': INDICATORS,
        'initial_amount': INITIAL_AMOUNT,
        'transaction_cost_pct': TRANSACTION_COST_PCT,
        'hmax': HMAX,
        'train_data_shape': train_df.shape,
        'test_data_shape': test_df.shape,
        'train_date_range': {
            'start': str(train_df['timestamp'].min()),
            'end': str(train_df['timestamp'].max())
        },
        'test_date_range': {
            'start': str(test_df['timestamp'].min()),
            'end': str(test_df['timestamp'].max())
        },
        'sac_config': config,
        'improvements': [
            f"Buffer size increased to {config['buffer_size']:,}",
            f"Learning starts reduced to {config['learning_starts']:,}",
            f"Gradient steps increased to {config['gradient_steps']}",
            f"Automatic entropy tuning: {config['ent_coef']}",
            f"SDE exploration: {config.get('use_sde', False)}",
            f"Total timesteps: {config['total_timesteps']:,}"
        ]
    }
    
    agent_info_path = os.path.join("models", "sac", f"{model_name}_info.pkl")
    with open(agent_info_path, 'wb') as f:
        pickle.dump(agent_info, f)
    print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• agent: {agent_info_path}")
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏£‡∏∏‡∏õ
    print(f"\nüìã ‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Enhanced SAC Agent:")
    print(f"üî§ ‡∏ä‡∏∑‡πà‡∏≠ Model: {model_name}")
    print(f"üèÜ Grade: {config['grade']} - {config['description']}")
    print(f"‚è±Ô∏è ‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å: {training_time}")
    print(f"üìÅ ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå: models/sac/")
    print(f"üì¶ ‡πÑ‡∏ü‡∏•‡πå Model: {model_name}.zip")
    print(f"üìÑ ‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {model_name}_info.pkl")
    
    return model_name, agent_info

def compare_with_original(improved_model, original_sac_path, test_env):
    """
    ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Improved SAC ‡∏Å‡∏±‡∏ö Original SAC
    """
    print(f"\nüìä ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û...")
    print("-" * 50)
    
    try:
        # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Improved model
        print("üß™ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Enhanced SAC...")
        improved_results = test_agent(improved_model, test_env, "Enhanced SAC")
        
        # ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏ó‡∏î‡∏™‡∏≠‡∏ö Original model (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
        if os.path.exists(original_sac_path):
            print("üß™ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Original SAC...")
            original_model = SAC.load(original_sac_path)
            original_results = test_agent(original_model, test_env, "Original SAC")
            
            # ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö
            print(f"\nüìà ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö:")
            print(f"{'Metric':<20} {'Original SAC':<15} {'Enhanced SAC':<15} {'Improvement':<15}")
            print("-" * 65)
            
            original_return = (original_results['final_value'] - INITIAL_AMOUNT) / INITIAL_AMOUNT * 100
            improved_return = (improved_results['final_value'] - INITIAL_AMOUNT) / INITIAL_AMOUNT * 100
            improvement = improved_return - original_return
            
            print(f"{'Total Return (%)':<20} {original_return:<15.2f} {improved_return:<15.2f} {improvement:<15.2f}")
            print(f"{'Final Value ($)':<20} {original_results['final_value']:<15.2f} {improved_results['final_value']:<15.2f} {improved_results['final_value'] - original_results['final_value']:<15.2f}")
            
        else:
            print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö Original SAC model ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö")
        
        return improved_results
        
    except Exception as e:
        print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö: {str(e)}")
        return None

def test_agent(model, test_env, model_name):
    """
    ‡∏ó‡∏î‡∏™‡∏≠‡∏ö agent ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    """
    obs, _ = test_env.reset()
    account_values = []
    actions_taken = []
    
    for step in range(len(test_env.df) - 21):
        action, _ = model.predict(obs, deterministic=True)
        obs, reward, done, truncated, info = test_env.step(action)
        
        account_values.append(info['total_value'])
        actions_taken.append(action[0])
        
        if done or truncated:
            break
    
    if account_values:
        final_value = account_values[-1]
        total_return = (final_value - INITIAL_AMOUNT) / INITIAL_AMOUNT * 100
        
        print(f"üí∞ {model_name} - ‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô: ${INITIAL_AMOUNT:,.2f}")
        print(f"üí∞ {model_name} - ‡πÄ‡∏á‡∏¥‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢: ${final_value:,.2f}")
        print(f"üìà {model_name} - ‡∏ú‡∏•‡∏ï‡∏≠‡∏ö‡πÅ‡∏ó‡∏ô‡∏£‡∏ß‡∏°: {total_return:.2f}%")
        
        return {
            'final_value': final_value,
            'total_return': total_return,
            'account_values': account_values,
            'actions_taken': actions_taken
        }
    
    return None

def main():
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô Enhanced SAC Agent
    """
    print("üöÄ Enhanced SAC (Soft Actor-Critic) Cryptocurrency Trading Agent")
    print("=" * 70)
    
    try:
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ Grade
        print("üîç ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ Grade...")
        
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö 48GB RAM + GPU ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ Grade A
        recommended_grade = 'A'
        
        ram_gb = 48  # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡πÉ‡∏™‡πà‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏ö‡∏ö
        gpu_available = torch.cuda.is_available()
        
        print(f"üíª ‡∏£‡∏∞‡∏ö‡∏ö: {ram_gb}GB RAM, GPU: {'‡∏°‡∏µ' if gpu_available else '‡πÑ‡∏°‡πà‡∏°‡∏µ'}")
        print(f"üèÜ ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ Grade: {recommended_grade} (Advanced)")
        
        # Setup device
        device = setup_device()
        
        # ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        df = load_existing_data()
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏° technical indicators
        df = add_technical_indicators(df)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á environment
        train_env, test_env, train_df, test_df = create_environment(df)
        
        # ‡∏ù‡∏∂‡∏Å Enhanced SAC agent
        trained_model, config, training_time = train_improved_sac_agent(train_env, grade=recommended_grade)
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å agent
        model_name, agent_info = save_improved_sac_agent(trained_model, config, training_time, train_df, test_df)
        
        # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö agent
        results = test_agent(trained_model, test_env, f"Enhanced SAC Grade {recommended_grade}")
        
        # ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö original (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
        original_sac_path = "models/sac/sac_agent_original.zip"  # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏£‡∏¥‡∏á
        compare_with_original(trained_model, original_sac_path, test_env)
        
        print("\nüéâ ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à! Enhanced SAC Agent ‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÅ‡∏•‡πâ‡∏ß")
        print("üîß ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏´‡∏•‡∏±‡∏Å:")
        for improvement in agent_info['improvements']:
            print(f"   ‚Ä¢ {improvement}")
        print("=" * 70)
        
    except Exception as e:
        print(f"\n‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}")
        print("üîç ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á")
        sys.exit(1)

if __name__ == "__main__":
    main() 